<p>POLITECNICO DI TORINO
Master's Degree
in Mechatronics Engineering
Master's Degree Thesis
Tra jectory Planning for Self-Driving Cars
Sup ervisor Candidate
prof. Stefano Malan Viglietti Claudia
Academic Year 2021-2022</p>

<p>The core challenge is to put a
safe and reliable automated
driving system on the road
Dr. Stephanonle
1</p>

<p>Abstract
This thesis aim s to achieve a b etter understanding ab out tra jectory and path plan-
ning in self-driving. The author participated to the Bosch Future Mobility Challe nge</p>

<p>2022 (BFMC 2022), a challenge prop osed by Bosch Romania in which students are</p>

<p>asked to develop autonomous driving and connectivity algorithms on 1 : 10 scaled
vehicles. In the comp etition, the car must p erform sp ec ic tasks:
1.
 lane follow;
2.
 lane ke eping;
3.
 intersection de tection;
4.
 trac sign and trac light recognition;
5.
 parking mano euvre;
6.
 overtake mano euvre;</p>

<p>7.
 ob jec t detection;
8.
 tra jec tory and path planning based on graphs and GPS connection.
At the b eginning, all memb ers of the team worked together to make the vehicle able
to fulll the rst 6 tasks. Next, each one had its own assignment and the author</p>

<p>job was studying the lo calisation system and tra jectory planning; path planning was</p>

<p>studied additionally after the comp etition to c omplete a metho dological study. The
ob jec tive of this work is to nd a predened path w he re the car is able to p erform
all the tasks required by the challenge and to go deep er into comprehend how to</p>

<p>nd the s hortes t path knowing the starting and nal p oints .
2</p>

<p>Contents
1 Intro duction 6
1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
 6
1.2 Autonomous driving . . . . . . . . . . . . . . . . . . . . . . . . . . .
 6</p>

<p>1.3 Leve ls driving automation . . . . . . . . . . . . . . . . . . . . . . . .
 8
1.4 Thesis outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
 8
2 Bosch Future Mobility Challenge 10
2.1 Intro duction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
 10
2.2 The Comp etition . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
 10</p>

<p>2.3 The Car-Kit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
 13
2.4 The Pro ject . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
 14
2.4.1 Comp etition Do cumentation and First Steps . . . . . . . . .
 14
2.4.2 Brain Pro ject . . . . . . . . . . . . . . . . . . . . . . . . . . .
 15</p>

<p>2.4.3 Emb edded Pro ject . . . . . . . . . . . . . . . . . . . . . . . .
 15</p>

<p>2.4.4 GITHUB . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
 16
2.5 The Struc ture b ehind the Algorithms . . . . . . . . . . . . . . . . . .
 16
3 Files communi cation 18
3.1 Parallelism, Thread and Pro cesses . . . . . . . . . . . . . . . . . . .
 18</p>

<p>3.2 The Main.py le . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
 21
3.3 Server Communication and UDP . . . . . . . . . . . . . . . . . . . .
 22
4 Technology b ehind autonomous driving 26
4.1 Sensors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
 26
4.2 Detailed study ab out sensors . . . . . . . . . . . . . . . . . . . . . .
 27
4.2.1 Came ra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
 27
4.2.2 LiDar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
 28
4.2.3 Ultrasonic . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
 31</p>

<p>4.2.4 ATM103 Enco der . . . . . . . . . . . . . . . . . . . . . . . . .
 34</p>

<p>4.2.5 IMU Sensor . . . . . . . . . . . . . . . . . . . . . . . . . . . .
 34
4.3 Lo calisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
 36
4.3.1 Lo calisation and mapping for BFMC2022 . . . . . . . . . . .
 37</p>

<p>4.3.2 V2X for BFMC2022 . . . . . . . . . . . . . . . . . . . . . . .
 42
5 Tra jectory and path planning 48
5.1 Tra jectory planning . . . . . . . . . . . . . . . . . . . . . . . . . . .
 48
5.1.1 Tra jectory planning used in comp e tition . . . . . . . . . . . .
 48
5.2 Path planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
 53
5.2.1 Dijkstra Algorithm . . . . . . . . . . . . . . . . . . . . . . . .
 56</p>

<p>5.2.2 A* Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . .
 60
6 Parking 66
6.1 Parking at comp etition . . . . . . . . . . . . . . . . . . . . . . . . . .
 66
7 Conclusion and future development 69
3</p>

<p>List of Figures
1 Timeline of autonomous vehicles . . . . . . . . . . . . . . . . . . . .
 7
2 Automation leve ls of So ciety of Automotive Engineers (SAE). . . . .
 8</p>

<p>3 Car of the team to the nals. . . . . . . . . . . . . . . . . . . . . . .
 9</p>

<p>4 PoliTron team logo. . . . . . . . . . . . . . . . . . . . . . . . . . . .
 10</p>

<p>5 Test Track. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
 11
6 Bosch Future Mobility Challenge 2022 - Timeline. . . . . . . . . . .
 12
7 Bosch Future Mobility Challenge 2022 - Be st New Participating Team
Award. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
 13
8 Car with the listed Comp onents. . . . . . . . . . . . . . . . . . . . .
 14</p>

<p>9 BFMC website - Layout of the Shared Do cumentation. . . . . . . . .
 14
10 Architecture of the completed pro ject. . . . . . . . . . . . . . . . . .
 17
11 Threads parallelism. . . . . . . . . . . . . . . . . . . . . . . . . . . .
 19</p>

<p>12 Comparison b etween TCP and UDP proto c ols. . . . . . . . . . . . .
 23</p>

<p>13 Fundamentals of UDP so cket programming. . . . . . . . . . . . . . .
 25</p>

<p>14 Autonomous Driving Sensors . . . . . . . . . . . . . . . . . . . . . .
 26
15 Pi Camera mo dule v2.1 . . . . . . . . . . . . . . . . . . . . . . . . .
 28
16 Schematic of ToF principle . . . . . . . . . . . . . . . . . . . . . . .
 30</p>

<p>17 LiDar mounte d on the car of the team. . . . . . . . . . . . . . . . . .
 30</p>

<p>18 Overtake mano euvre . . . . . . . . . . . . . . . . . . . . . . . . . . .
 33</p>

<p>19 IMU sensor BNO055 . . . . . . . . . . . . . . . . . . . . . . . . . . .
 35
20 V2V real life representation . . . . . . . . . . . . . . . . . . . . . . .
 37
21 Table of No de and Connections [1]. . . . . . . . . . . . . . . . . . . .
 38</p>

<p>22 Comp etition track with no des and connections [1]. . . . . . . . . . .
 39</p>

<p>23 Example of tra jectories in autonomous vehicle [2]. . . . . . . . . . .
 48</p>

<p>24 Comp etition track with the designed path. . . . . . . . . . . . . . . .
 50
25 Car of the team at intersection with grades of IMU s ensor sp ecied.
 52
26 Example of di
erent paths on the same m ap. . . . . . . . . . . . . .
 53</p>

<p>27 Sampling based approache s with the main advantages and drawbacks
[3] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
 55
28 No de-based optimal approaches with the main advantages and draw-
backs [3] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
 56
29 Flowchart of Dijkstra algorithm . . . . . . . . . . . . . . . . . . . . .
 57</p>

<p>30 Autonomous parking. . . . . . . . . . . . . . . . . . . . . . . . . . .
 66</p>

<p>31 Autonomous parking in the comp etition. . . . . . . . . . . . . . . . .
 67
4</p>

<p>List of Tables
1 Semaphore State [1] . . . . . . . . . . . . . . . . . . . . . . . . . . .
 43
2 ID assignment for each obstacle [1] . . . . . . . . . . . . . . . . . . .
 44
5</p>

<p>1 Intro duction
1.1 Motivation
Technology advanc es always faster and daily, humans use every kind of electronic
devices for various purp oses reducing e
ort, lost time and danger. The pro duction of</p>

<p>autonomous vehicles, rob ots or aircraft b ecomes a leading industry. The innovations</p>

<p>are helpful to improve b oth p erformance and safety.
The motivation of the thesis is to design a driver-less vehicle following the rules
of the Bosch Future Mobility challenge. In particular, automated vehicle, rob ot or</p>

<p>aircraft can move from a designated starting p oint to a destination one in a given</p>

<p>environment. In order to reach the nal p oint, a path must b e planned and there</p>

<p>are more metho ds to achieve the goal. The following approach are studied in deep:
â€¹
Tra jectory planning is useful to design a predened path that autonomous</p>

<p>system must follow in relation to a dened time law. For example, autonomous
car can p erform a limited set of p ossible maneuvers and the planner has to
help it to keep the lane, parking or overtake.
â€¹
Path planning is used to complete the task in the shortest time with mini-</p>

<p>mum move ment, save energy and improve eciency as well. For instance, the</p>

<p>planner can try to cho ose the fastest path for an autonom ous cart that has to</p>

<p>transp orts go o ds and deliver them.
Both metho ds are studied on a known environment given by Bosch.
1.2 Autonomous driving
Autonomous driving can provide a fundamental contribution to the s olution of
greater road safety and climate change. A study of Waymo Simulated Driving</p>

<p>Behavior in Re construc ted Fatal Crashes within an Autonomous Vehicle Op erating
Domain illustrates that the Waym o autonomous car preve nts a collision with an
estimated of 82% b etter than an human driver, it reduces 10% of crash-level serious</p>

<p>injury risk and it has the same b ehaviour of a human driver with an estimate d of</p>

<p>8% [4].
The World Health Organization evaluated that almost 1.3 million p eople die
each year due to road trac crashes. A 2015 National Highway Trac Safety Ad-</p>

<p>ministration rep ort found that human error causes 94 p ercent of trac accidents [5].</p>

<p>Numb er that is likely to decrease considerably b ecause computers have no emotion,</p>

<p>they can not drink or b eing distracted consequentially they reduce and eliminate the</p>

<p>op erator errors. Self-driving development dep end ab ove p eople's trust and reliable
solutions and in order to work, a vehicle has to b e able to p erceive and understand its
surroundings (\Sense"), pro cess the information, plan a driving strategy (\Think"),</p>

<p>and safely imple ment the planned driving strategy (\Act").
Leonardo da Vinci (1500) des igne d a cart that is c ons ide re d the world's rst
rob ot. The cart could move along a predetermined path without b eing pushed or</p>

<p>pulled; p ower is provided by springs under high tension and s te ering is s et up in
advance.
Rob ert Whitehead (1869) invented a torp e do that prop elled itself underwater
thanks to a pressurization system called \The Secret" for several hundred yards
underwater and maintained depth. Torp edo guidance led to a wide range of au-</p>

<p>tonomous devices.
6</p>

<p>Going forward, te chnology and knowledge advance d and autonomous pro jects
evolved (Fig. 1).
Nowadays, a lot of ve hic le s are considered se mi-autonomous b e cause they have a
wide range of safety features like braking syste ms, assisted parking and the ability to
drive, brake, stee r and park themselves. Their technologies rely on GPS capabilities</p>

<p>and s ensing systems to detect lanes, obstacles, road signs and trac lights. Com-
panies are racing to build autonomous vehicles for a radically changing consumer
world.
Figure 1: Timeline of autonomous vehicles
7</p>

<p>1.3 Levels driving automation
The So ciety of Automotive Engineers (SAE J3016, 2018) provide six levels driving
automation from Level 0 to Le vel 5, from fully manual driving to fully autonomous
(Fig. 2):
â€¹
Level 0 (No Driving Automation): vehicles are manually controlled. Human</p>

<p>drivers control all the dynamic tasks asso ciated with driving.
â€¹
Level 1 (Driver Assistance): human drivers are in full control of the vehicle but
they are assisted by the single automated system (steering or accelerating).
â€¹
Level 2 (Partial Driving Automation): the vehicle can control b oth steering</p>

<p>and accelerating/decelerating through advanced driver assistance systems or</p>

<p>ADAS but driver can still manage the car.
â€¹
Level 3 (Conditional Driving Automation): vehicles can take some decisions</p>

<p>such as emergency braking without human judgment using sensors such as</p>

<p>LiDar. They still require hum an that must b e on standby in case the system
is unable to execute the task.
â€¹
Level 4 (High Driving Automation): vehicles can make decisions and can in-</p>

<p>tervene if there is a system failure. They do not need human interaction in
most circumstances but driver s till has option to manually override and they</p>

<p>can move only within a limited area. This is known as geofencing.
â€¹
Level 5 (Full Driving Automation): vehicles do not require human interactions</p>

<p>and they can go anywhere b eing free from ge ofe nc ing.
Figure 2: Automation levels of So ciety of Automotive Engineers (SAE).
1.4 Thesis outline
In this thesis, tra jectory planning and path planning are s tudied in deep. Chapters
are organised as follows:
Chapter 2
: Intro duction to Bosch Future Mobility Challenge (Fig. 3) and details
ab out the pro ject, including software and hardware comp onents.
8</p>

<p>Chapter 3
: Explanation of the communication b etween les, including crucial
Python utilities and communication with LAN and UDP.
Chapter 4
: Information ab out technology b ehind autonomous driving with a de ep-
ening on sensors and lo calization system. Description of sensors, explanation
of the lo calisation system and corresp onding algorithms used for comp etition.
Chapter 5
: Technicality and algorithms ab out tra jectory and path planning. Ex-
planation of tra jectory planning algorithm used in comp etition. Deep ening of</p>

<p>Dijkstra algorithm and A* algorithm and co de explanation.
Chapter 6
: Intro duction to the parking task and description of parking algorithm
used for the challenge.
Chapter 7
: Conclusions and future development ab out the work are rep orted.
Figure 3: Car of the team to the nals.
9</p>

<p>2 Bosch Future Mobility Challenge
2.1 Intro duction
This work has b een realized based on the e
 ort and the assignments p erformed dur-
ing the participation to the so-c alle d
Bosch Future Mobility Chal lenge
(BFMC): it
is an international autonomous driving and connectivity comp etition for bachelor</p>

<p>and master students organized by
Bosch Engineering Centre Cluj
since 2017. The
comp etition invites stude nt teams from all over the world every year to develop au-</p>

<p>tonomous driving and connectivity algorithms on 1 : 10 scaled vehicles, provided by</p>

<p>the company, to navigate in a designated environment simulating a miniature smart</p>

<p>city. The students work on their pro jects in collab oration with Bos ch exp erts and</p>

<p>academic professors for several months to develop the b est-p erforming algorithms.
The author of this work has joined the challenge under the team
PoliTron
(Fig.
4) comp osed by 4 other colleagues from the master's degree program in Mechatronic</p>

<p>Engineering of the Polyte chnic of Turin, with the guidance of the sup ervisor himself,</p>

<p>Professor Stefano Malan.
Figure 4: PoliTron team logo.
The job to carry out during the challenge, which lasts from Novemb er to May,
consists in developing the algorithms involved in the realization of the autonomous</p>

<p>car guide and implementing them into the received car, therefore it commits b oth the
software and the hardware parts. All in all, it is a real and complete accomplishment
of self-driving c ar.
2.2 The Comp etition
The comp etition require s that, in addition to the activities carried out by the teams
to achieve the nal ob jective, participating teams send a monthly p erio dic status
via the comp etition website containing the followings to show their progress to the
Bosch representatives:
â€¹
A
technical rep ort
describing the development in the last sprint.
â€¹
A
pro ject plan
alongside with the
pro ject architecture
.
â€¹
A
video le
emphasizing with visual aid the contributions from the past
month activity (already present in the rep ort and pro ject plan).
In the middle of the c omp etition, on middle March, a rst eliminatory phase
takes place, the Mid-Season Quality Gate, in which each team is requested to send</p>

<p>a 3-minutes (at most) long video in which the car must p erform the following tasks</p>

<p>in a single autonomous run:
1.
 Lane keeping.
2.
 Intersection crossing.
10</p>

<p>3.
 Complete mano euvre after the following signs:
3.1.
Stop
sign { the car must stop for at least 3 s .
3.2.
Crosswalk
sign - the car must visibly reduce the sp eed and if a p edestrian
is crossing, the car must stop.
3.3.
Priority Road
sign - act normal, you are on a priority road and any vehicle
coming from a non-priority road should stop.
3.4.
Parking
sign - it is found b efore and afte r a parking lot and, if empty,
can b e use d to p erform the parking mano euvre.
These tasks can b e demonstrated by means of one of three p ossible alternatives:
â€¹
A video of the car p erforming the actions on a real-life like map.
â€¹
A video of the car in front of a Desktop, taking a video as a simulated input
and acting accordingly.
â€¹
A video of the car in front of a Desktop where the simulator is running, taking
as visual input the one from the camera ins ide the simulator.
The author's team has chos en the rst option, realizing physically the track
shown in Figure 5.
Figure 5: Test Track.
Based on the videos, the jury will decide which teams p ossess the right skills to
continue the comp etition and to go to the Bos ch Engineering Centre site in Cluj-</p>

<p>Nap o c a (Romania) for the qualications and p ossibly seminals and nals in May.
During the race p erio d in Romania, the teams will have to face two challenges:
the technical and the sp eed one. The former requests that the car can correctly
resp ect most of the road signs, such as trac signs, trac lights, lanes, intersections,</p>

<p>ramps, and roundab outs. Moreover, it must detect p edestrians and overtake other
cars present in the same lane. The latter asks the car to complete a determined
11</p>

<p>path in the shortest time p ossible, this time resp ecting only the lanes and the road
markings. In addition to this, the teams will make a presentation in front of the</p>

<p>jury.
Only a maximum of 8 teams will b e selected to participate to the nal race , in
which the rst 3 qualied teams will win b oth a money prize and the c ar kit, and</p>

<p>another team, not included in the top 3, will b e rewarded as the \b est ne wcomer",
meaning a team which did not take part to the c omp etition in the previous year.
All the phas es of the challenge are rep orted in Figure 6.
Figure 6: Bosch Future Mobility Challenge 2022 - Timeline.
The author's team managed to reach the nals and comp eted with other 7 tal-
ented teams from Greece, Romania, Portugal and Italy and won the Best new par-</p>

<p>ticipating team award (Figure 7).
12</p>

<p>Figure 7: Bosch Future Mobility Challenge 2022 - Best New Participating Team
Award.
2.3 The Car-Kit
Going into the details of the car kit provided by Bosch, the following comp onents
are to b e found:
â€¹
Nucleo F401RE.
â€¹
Raspb erry Pi 4 Mo del b.
â€¹
VNH5012 H-bridge Motor Driver.
â€¹
ATM103 Enco der.
â€¹
DC/DC converters.
â€¹
Servomotor.
â€¹
LiPo Battery.
â€¹
Chassis.
â€¹
Camera.
â€¹
IMU Sensor.
The fundamental comp onents are shown in Figure 8.
13</p>

<p>Figure 8: Car with the listed Comp onents.
In addition to these basic elements, the team decided to furnish the car with
a LiDAR sensor and an Ultrasonic sensor, placed resp ectively in the front and the
right-hand side of the car.
2.4 The Pro j ect
To start working on the pro ject, the teams are provided with a complete do cumen-
tation necessary to understand b etter the structure of the pro ject, esp ecially the
hardware side and the base Python/C++ co des for the correct communication of</p>

<p>all the comp onents of the car. The cited do cumentation is sub divided as shown in</p>

<p>Figure 9.
Figure 9: BFMC website - Layout of the Shared Do cumentation.
A brief explanation of the c ontent of each section is rep orted in the following
sub chapters.
2.4.1 Comp etition Do cumentation and First Steps
It includes:
â€¹
Connection diagram and description with ocial links to the comp onents of
the car.
14</p>

<p>â€¹
Racetrack: the description of the provided racetrack and its eleme nts, the
given comp onents , and the diagrams, as well as a starting p oint and directions</p>

<p>of the knowledge required.
â€¹
V2X-Vehicle to e verything: it includes lo c aliz ation, semaphore, environmental
server, and vehicle-to-vehicle communication.
â€¹
Printed com p onents and circuit b oards.
â€¹
Hardware improvements: it includes settings for the hardware comp one nts.
â€¹
Useful links for Raspb erry Pi, ROS, and Python.
â€¹
Perio dic status: pro ject plan and architecture, rep orts and media.
2.4.2 Brain Pro ject
The Brain Pro ject describ es the given co de for the RPi platform. It includes the
start-up co de and the do cumentation for the provided API's, which will help the</p>

<p>interaction with the V2X systems. T he pro ject uses concepts of multi-pro cessing</p>

<p>and distributed s ystem, and it implements a basic 
exible structure, which can b e
extended with new features. This folder contains:
â€¹
Intro duction: concept and architectures, in particular remote car control and
camera stream ing, installation and conguration, IMU displayer.
â€¹
Utils layer: camera streamer, remote control.
â€¹
Hardware layer: came ra, serial handler pro cess and camera sp o ofer pro cess.
â€¹
Data acquisition layer: trac lights, lo calization system, environmental server.
The com puter pro ject is already implemented on the provided Raspb erry Pi,
while the emb edded pro ject is alre ady implemented on the Nucleo b oard. Together,
they give a go o d starting p oint for the pro ject, providing a remote keyb oard control,
remote c amera stream, constant sp eed control of the given kit and others.
2.4.3 Emb edded Pro ject
This do cumentation describ es the low-level application which runs on the micro-
controller Nucleo-F401RE. It aims at controlling the car movement and providing</p>

<p>an inte rface b etween higher level controllers and lower-level actuators and sensors.
The pro ject has four parts:
â€¹
To ols for development containing the instructions to upload the co des related</p>

<p>to the correct functioning of the Nucleo.
â€¹
Brain layer contains the state machine of the Nucleo (sp eed and steering).
â€¹
Hardware package inc ludes the drivers for actuator and s ensors.
â€¹
Signal, utils and p erio dics namespace: `signal' includes libraries for pro cess ing</p>

<p>signals, <code>utils' package incorp orates some util functionalities and</code>p erio dics'</p>

<p>layer includes some p erio dic tasks.
15</p>

<p>2.4.4 GITHUB
Bosch provided their own link of GitHub in which all the Python/C++ co des related
to the topics describ ed ab ove are held. Sp ecically:
â€¹
Brain
and Brain
ROS: the pro ject includes the software already pre sent on
the development b oard (Raspb erry Pi) for c ontrolling the car remotely, use</p>

<p>the API's and test the simulated s ervers, resp e ctively for Raspbian and ROS.
â€¹
Startup_C
: the pro ject includes some of the scripts transcrib ed in C++ lan-
guage from the startup pro ject.
â€¹
Embedded_Platform
: the pro ject includes the s oftware already present on
the Emb edded platform (Nucleo b oard). It describ es all the low-level software</p>

<p>for controlling the sp eed and stee ring of the car.
â€¹
Simulator
: the pro ject includes the software for the Gazeb o simulator, which
is the ocial on-line environme nt of the comp e tition.
â€¹
Documentation
: the pro jec t includes all the general do cumentation of the
comp etition environment, guides, diagrams, car comp one nts, etc.
2.5 The Structure b ehind the Algorithms
The tasks to p erform by the end of the comp etition are the following:
â€¹
Lane Keeping and Following.
â€¹
Intersection Detection and crossing.
â€¹
Correct m ano euvres under the dete ction of the following trac signs: stop,
priority, crosswalk, parking, roundab out, highway entrance and highway exit,
one-way, no entry.
â€¹
Parallel and p e rp endicular parking.
â€¹
Ob je ct Detection: p edestrian and overtake of a static and/or moving vehicle.
â€¹
Navigation by means of no des and lo calization system (GPS).
The brain of the car must b e inserted in the Raspb erry Pi which, basing on the
tasks to p erform, sends the commands to the Nucleo which, in turn, acts on the
motor and on the servo motor to regulate b oth the sp eed and stee r. More in details,</p>

<p>in order to pro cess the image, the Raspb erry takes as input the camera frame and</p>

<p>the IMU data for the p osition of the vehicle, runs the sp ecic control algorithms and
sends the corresp onding output commands to the Nucleo; for example, an increased
sp eed in pres ence of a ramp which signs the entrance to the highway, a decreased</p>

<p>sp eed and a sp ecic s teer when traveling along a tight curve and a zero sp eed when</p>

<p>the tra c light turns red. The correlation b etween all the pro ject comp onents,</p>

<p>the sensors, the algorithms and the vehicle actuation is represented in the pro ject
architecture shown in Figure 10.
16</p>

<p>Figure 10: Architecture of the completed pro ject.
The pro ject presented in this chapter sinks the ro ots for the work develop ed
by three memb ers of te am PoliTron: sp ecically, Cristina Chicca deals with the
image pro ce ssing part, Gianmarco Picariello's work consists in the development of</p>

<p>MPC controllers in this conte xt and Claudia Viglietti's the sis concerns optimization</p>

<p>algorithms for path planning.
17</p>

<p>3 Files communication
Before diving into the algorithms de aling with the pap er matter, it is of particular
interest to dene appropriately how the les shown in Figure 10 communicate, b oth</p>

<p>by me ans of Python3 and using a given se rver resp onsible for the car lo calisation</p>

<p>system.
3.1 Parallelism, Thread and Pro cesses
The whole pro ject is comp osed by pro cesses and threads which ensure that all the
algorithms present on the Raspb erry Pi run correctly and in parallel since the car,</p>

<p>to p erform a correct self-drive, needs to execute them concurrently: for example, it</p>

<p>has to always follow the lane while resp ecting trac and road signs, checking for</p>

<p>p edestrians crossing the street etc.
Python multipro cessing library o
ers two ways to implement pro ces s-based par-
allelism:
â€¹
Pro cess: used when functions-based parallelism is required.
â€¹
Po ol: o
ers a convenient means of parallelizing the execution of a function</p>

<p>across multiple input values, distributing the input data across pro cesses (data-</p>

<p>based parallelism).
In this pro ject cas e, the
Pro cess
metho d has b ee n use d: when it is run, it has
a sp ecic address on the memory and all the used variables are accessible only by</p>

<p>the same pro ce ss, so they cannot b e read by another pro cess unless a pip e is used.
A
Pip e
is a metho d to pass information from one pro cess to another one: it o
ers
only one-way communication and the pass ed information is held by the system until
it is read by the receiving pro cess. What is returned from the
pipe()
function is a
pair of le descriptors (r,w) usable for reading and writing resp e ctively.
In addition to the Pro cess mo dule, the multipro cessing mo dule o
ers the thread-
ing mo dule. The
Thread
class represents an activity that is run in a s eparate thread
of control. This function represents the capability of Python to handle di
erent tasks
at the same moment: to sum up, it is a se parate 
ow of execution in the sense that
Python script app ears to have more threads happ ening at once. Its syntax is the
following:
Thread
(group=None, target=None, name=None, args=(), kwargs=, *, dae-
mon=None)
In particular,
target
is the callable ob jec t to b e invoked by the
run()
metho d,
name
is the thread name and
args
is the argument tuple for the target invo c ation.
Moreover, the
.daemon
prop erty ensures that the daemon thread do es not blo ck the
main thread from exiting and continues to run in the background.
Multiple threads work in parallel as shown in Figure 11.
18</p>

<p>Figure 11: Threads parallelism.
In order to give an idea of how all these functions are relate d to one another
inside a working script, an example in the context of autonomous drive follows: it
is necessary to set the commands to send to the STM32 Nucle o micro controller on
a dedicated pro cess and, by means of a pip e, these commands are sent to the
Seri-
alHand lerProcess
, which deals with the c ommunication with the Nucleo. It imple-
ments the
WriteThread
function to send commands to Nucleo and the
ReadThread
function to receive data from Nucleo. The commands are generated after having
pro cessed the image s which come from the
CameraProcess
implementing the Rasp-
b erry Pi Cam and they are sent to the
LaneDetectionProcess
, res p onsible for the
lane detection. This means that
LaneDetectionProcess
has to receive two pip es, one
for receiving the images and one for sending the commands.
The pro ject contains many pro ce sses, each dening a particular algorithm for
the self-driving control: the denitions explained until now are useful to outline a
structure in common with all the se pro cesses, which is shown b elow.
class
 Name(WorkerProcess):
def
 <strong>init</strong>(self, inPs, outPs):
"""
 Process
 used
 for
 sending
 images
 over
 the
 network
 to
 a
 targeted
IP
 via
 UDP
 protocol
 (
no
 feedback
required
)
.
 The
 image
 is
 compressed
 before
 sending
 it
.
Used
 for
 visualizing
 your
 raspicam
 images
 on
 remote
 PC
.</p>

<h2>Parameters</h2>

<p>inPs
 :
 list
(
Pipe
)
List
 of
 input
 pipes
,
 only
 the
 first
 pipe
 is
 used
 to
 transfer
 the
captured
 frames
.
outPs
 :
 list
(
Pipe
)
List
 of
 output
 pipes
In
 this
 section
 you
 can
 also
 define
 the
 variables
 initialization
.
"""
super
(MainLaneDetectionProcess, self).<strong>init</strong>(inPs, outPs)
19</p>

<p>def
 run(self):
"""
 Apply
 the
 initializing
 methods
 and
 start
 the
 threads
.
 """
super
(MainLaneDetectionProcess, self).run()
def
 <em>init</em>threads(self):
"""
 Initialize
 the
 sending
 thread
.
 """</p>

<p>#
 Thread
 for
 elaborating
 the
 received
 frames
:
receiveFrameT = Thread(name=
'
receiveFrameThread
'
,
target=self.<em>generate</em>Output,
args=(self.inPs, self.outPs,))
receiveFrameT.daemon = True</p>

<p>self.threads.append(receiveFrameT)
After creating the class whos e name is sub jective (in this c ase it is called
Name
),
the utilized functions and metho ds are the following:
â€¹
init
: takes as input
self
,
inPs
and
outPs
which corresp ond to the
list of input pip es and the list of output pip es resp ectively. This function is
called w he n a class is \instantiate d", meaning when the class is declared and
any argume nt withing this func tion will also b e the same argument when in-</p>

<p>stantiating the class ob ject. These initial arguments are the data manipulated</p>

<p>throughout the class ob ject. Under this function, some instance attributes are</p>

<p>dened and assigned to
self
to b e manipulated later on with other functions.
â€¹
super()
: it inherits, uses co de from the base (existing) class (i.e.,
Worker-
Process
) to dene the structure of the new class (i.e.,
Name
) { it guarantees
the access metho ds from a parent class within a child class reducing rep etitions</p>

<p>in the co de.
â€¹
run()
: function that initializes the sending thread for the pro cessing of re -
ceived frames.
â€¹
.append()
: adds a single item to the existing list. It do es not return a new
list of items, but it will mo dify the original list by adding the item to the
end of the list. After executing the metho d
append
on the list, the list size
increases by one.
All of them send their output to a particular pro cess called
MovCarProcess
: it is
resp onsible for setting the correct values of ste er and sp e ed of the car according to
the road situation, e.g the value of the lane curve, the detected trac sign, the inter-</p>

<p>section etc.. Thes e value s are integers repres entative of the mano euvre: for example,</p>

<p>a value of 999 corresp onds to sp eed equal to 0 in the
SpeedThread
. Summarizing,
MovCarProcess
sets the representative value according to the output received from
the control pro cesses, whereas
SpeedThread
and
SteerThread
contain the actual com-
mand sent to the Nucleo for, resp e ctively, sp eed (action 1) and s teer (action 2). An</p>

<p>example is given by the co de shown b elow, in which the
MovCarProcess
sets the
value by means of which the car stops in pres ence of a STOP or CROSSWALK sign</p>

<p>and b oth
SpeedThread
and
SteerThread
actually build the physical command.
"""
 Extract
 from
 MovCarProcess
 """
if
 STOP
 or
 CROSSWALK:
20</p>

<p>value = 999
"""
 Extract
 from
 SpeedThread
 """
#
Stop
if
 curveVal == 999:
command = {
'
action
'
:
 '
1
'
,
 '
speed
'
: 0.0}
"""
 Extract
 from
 SteerThread
 """
#
Stop</p>

<p>if
 curveVal == 999:
command = {
'
action
'
:
 '
2
'
,
 '
steerAngle
'
: 0.0}
Similarly, these threads will set sp eed and steer values di
erent from 0 whenever
the car has to travel along the path, in absence of road and trac signs that would
imp ede it.
3.2 The Main.py le
All the pro cesse s which have to b e run on the Raspb erry Pi, including their inputs
and outputs, the way in which they communicate , are describ ed inside the
main.py
le. As every main function, it has the job to put together the functions involved in</p>

<p>the autonomous-driving solution, searching them from their sp ecic folder.
ArcShRead, ArcShSend = Pipe(duplex=False)
 #
 for
 serial
 handler
FrameRead1, FrameSend1 = Pipe(duplex=False)
 #
 Frame
 towards
 Lane
Detection
FrameRead2, FrameSend2 = Pipe(duplex=False)
 #
 Frame
 towards
Intersection
 Detection
FrameRead3, FrameSend3 = Pipe(duplex=False)
 #
 Frame
 towards
 Sign
Detection
FrameRead4, FrameSend4 = Pipe(duplex=False)
 #
 Frame
 towards
Localization
 Process
#</p>

<h6>#</h6>

<p>IMAGE
 PROCESSING
 ALGORITHMS
 ##########
curveValRead, curveValSend = Pipe(duplex=False)
IntersectionRead, IntersectionSend = Pipe(duplex=False)
SignDetRead, SignDetSend = Pipe(duplex=False)
#</p>

<h6>#</h6>

<p>LOCALISATION
 ALGORITHMS
 ##########
LocalizationRead1, LocalizationSend1 = Pipe(duplex=False)
#</p>

<h6>#</h6>

<p>PROCESSES
 ##########
AshProc = SerialHandlerProcess([ArcShRead], [])
 #
receives
 the
 data
 from
MovCar
 and
 sends
 it
 to
 the
 Nucleo
allProcesses.append(AshProc)
AcamProc = CameraProcess([], [FrameSend1, FrameSend2, FrameSend3,
FrameSend4])
allProcesses.append(AcamProc)
ALaneProc = MainLaneDetectionProcess([FrameRead1], [curveValSend])
allProcesses.append(ALaneProc)
AInterProc = IntersectionDetectionProcess([FrameRead2],
21</p>

<p>[IntersectionSend])
allProcesses.append(AInterProc)
ASignProc = SignDetectionProcess([FrameRead3],[SignDetSend])
allProcesses.append(ASignProc)
AtrajProc = RaceTrajectoryProcessSO([FrameRead4], [LocalizationSend1])
allProcesses.append(AtrajProc)
AEnvProc = EnvironmentalProcessSO([LocalizationRead1], [])
allProcesses.append(AEnvProc)
AcurveValProc = MovCarProcess([curveValRead, IntersectionRead,
SignDetRead, LocalizationRead1],[ArcShSend])
allProcesses.append(AcurveValProc)
The example ab ove shows an extract from the
main.py
le: a
pip e
is dened for
every pro cess which has to receive the frame from the c amera as input (in this case,
there are 4 pro cesses which require it) and also, a pip e for lo calisation and se rial han-</p>

<p>dler data is dened. Then, every pro cess is declared, in the rst brackets the inputs</p>

<p>are listed, whereas in the second brackets the outputs are listed.
CameraProcess
has
no input but only the frames to se nd as output, whereas
SerialHand lerProcess
has
the output of
MovCarProcess
as input and no output. It is imp ortant to highlight
that not all the pro cesses rece ive as input the camera frames:
EnvironmentalPro-
cess
, resp onsible for sending the encountered ob jects to the server, rec eives as input
the co ordinates of the car from the
RaceTrajectoryProcess
, whereas
MovCarProcess
receives the inputs from the other pro ce sses (car lo calisation and ob jects detection)</p>

<p>and sends the commands to
SerialHand lerProcess
.
3.3 Server Communication and UDP
The car has an indo or lo calisation system which detects and sends by
UDP con-
nection
the relative p osition of itself and other cars prese nt on the race track.
The UDP communications describ e the programming for the User Datagram
Proto col provided in the TC P/IP to transfer datagrams over a network. Informally,
it is called "Send and Pray" b ecause it has no handshake, session or reliability,</p>

<p>meaning it do es not verify that the proto col has reached the destination b efore it</p>

<p>sends data. UDP has a 8-byte header that includes source p ort, destination p ort,</p>

<p>packet length (header and data) and a simple (and optional) checksum.
The checksum, when utilized, provides limited integrity to the UDP header and
data since it is simply an algorithm-base d numb er created b efore data is sent to</p>

<p>ensure data is intact once received: this pro cedure is done by running the same</p>

<p>algorithm in the received data and comparing the numb er b efore and after the</p>

<p>reception.
UDP avoids the ove rhe ad asso ciated with connections, error checks and retrans-
mission of missing data, it is suitable for real-time or high p erform anc e applications</p>

<p>that do es not require data veric ation or correction. In fact, the IP network delivers</p>

<p>datagrams that can b e up to 65507 bytes in le ngth but do es not guarantee that they</p>

<p>are delivered at the destination and in the same order as they are sent. Moreover,
UDP provides pre-pro cess addressing through p orts where IP provides addressing
of a sp ecic host. The pro cess is describ ed as follows:
22</p>

<p>1.
 These p orts are 16-bit values used to distinguish di
erent senders and re ceivers
at each end p oint.
2.
 Each UDP datagram is addressed to a sp ecic p ort at the end host and in-
coming UDP datagrams are demultiplexe d b etween the recipients.
The advantage of using UDP is the absence of retransmission delay, meaning it
is fast and suitable for broadcast. The disadvantage regards no guarantee of packets
ordering, no verication of the readiness of the receiving computer and no protection
against duplicate packets. Anyway, UDP is often us ed for streaming-typ e devices
such as lidar sensors, cameras and radars since there is no reason to re send data if</p>

<p>it is not received. Moreover, due to high data rates, resending pas t and corrupted</p>

<p>data would slow things down tremendously. A c omparison b etween TCP and UDP</p>

<p>is give n by Figure 12.
Figure 12: Comparison b etween TCP and UDP proto cols.
The connection b etween the c ar and the server is validated by means of the
API
communication
, which ensures the reading of the car given ID toge the r with a
certain p ort re sp onsible for the communication of the co ordinates of all the moving</p>

<p>obstacles. An API communication is a typ e of Application Programming Interface
which adds communication channels to a particular software. It allows two pieces
of software hosted on the cloud to connect to each other and transfer information.
An example of the UDP proto col used inside the pro ject is given by a le re-
sp onsible for reading the p osition of the c ar in real time (
position
listener.py
).
import
 sys
sys.path.insert(0,
'
.
'
)
import
 socket
import
 json</p>

<p>from
 complexDealer
 import
 ComplexDecoder
23</p>

<p>class
 PositionListener:
"""
PositionListener
 aims
 to
 receive
 all
 message
 from
 the
 server
.
"""</p>

<p>def
 <strong>init</strong>(self, server<em>data, streamPipe):
self.<strong>server</em>data = server<em>data
self.</strong>streamP</em>pipe = streamPipe
self.socket_pos = None</p>

<p>self.<strong>running = True
def
 stop(self):
self.</strong>running = False
try
 :
self.<strong>server<em>data.socket.close()
except
:
 pass
def
 listen(self):
while
 self.</strong>running:
if
 self.<strong>server</em>data.socket != None:
try
:
msg = self.</strong>server_data.socket.recv(4096)
msg = msg.decode(
'
utf
-8
'
)
if
(msg ==
 '
'
):
print
(
'
Invalid
 message
.
 Connection
 can
 be
 interrupted
.
'
)</p>

<p>break
coor = json.loads((msg),cls=ComplexDecoder)</p>

<p>self.<em>_streamP</em>pipe.send(coor)
except
 socket.timeout:
print
(
"
position
 listener
 socket_timeout
"
)</p>

<p>#
 the
 socket
 was
 created
 successfully
,
 but
 it
 wasn
'
t
 received
any
 message
.
 Car
 with
 id
 wasn
'
t
 detected
 before
.
pass
except
 Exception as e:
self.<em>_server</em>data.socket.close()</p>

<p>self.<em>_server</em>data.socket = None</p>

<p>print
(
"
Receiving
 position
 data
 from
 server
 "
 +
str
(self.<em>_server</em>data.serverip) +
 "
 failed
 with
 error
:
 "</p>

<p>+
 str
(e))
self.<em>_server</em>data.serverip = None</p>

<p>break
self.<em>_server</em>data.is<em>new</em>server = False</p>

<p>self.<strong>server<em>data.socket = None
self.</strong>server</em>data.serverip = None
Similarly to the Pro cess ob ject, the class
PositionListener
is comp osed by
the main functions
init
,
stop
and
listen
. In this cas e, the variables of
interest are
server
data
,
streamPipe
and
socket
.
A network so cket is a software structure within a no de of a computer network that
serves as an endp oint for s ending and re ceiving data. The structure and prop erties of
a so cket are dened by an API for the networking architecture. So ckets are created
24</p>

<p>only during the lifetime of a pro cess of an application running in the no de.
The function
listen
p erforms the following steps:
1.
 After the subscription on the server, it is listening the messages on the previ-
ously initialized so cket.
2.
 It deco des the messages and saves in 'co or' memb er parameter.
3.
 Each new message will up date the 'co or' parameter and the server will send the
result (car co ordinates) of last dete ction. If the car has b een detected by the
lo calization system, the client rece ives the sam e co ordinates and timestamp.
The UDP so cket programming fundamentals are represented by Figure 13.
Figure 13: Fundamentals of UDP so cket programming.
25</p>

<p>4 Technology b ehind autonomous driving
Autonomous vehicles (AVs) use "sense-plan-act" design. AVs are equipp ed by sen-
sors like cam era, LiDar, ultrasonic, radar and infrared to sense the environment (Fig.</p>

<p>14). A range of sensors in combination can b e complementary and comp ensate for</p>

<p>any weaknesses in any other sensor.
Sensors can degrade their p erformance b ecause of the ir limitations and inade -
quacies. Errors are due to drifting errors, surface irregularities, wheel slipping, low</p>

<p>sensor resolution or uncertainty in readings . Better is the accuracy of the sensors</p>

<p>fewer are the limitations and higher are the costs [6].
To plan, autonomous cars can use a blend of the Global Positioning System
(GPS) and Inertial Navigation Systems (INS) so that the vehicle can lo calize its
p osition. Both GPS and INS can have some unce rtainties and they can b e inac curate,
for this reason it is imp ortant to take into account their limits. After simulations</p>

<p>and eld testing, imp ortant parameters are set and control is managed through</p>

<p>rule-based controllers. The drawback is the diculty to generalize new scenarios, the</p>

<p>huge time required to tune the parameters and the non linear b ehavior of driving that
imply linearization of the vehicle mo del. Technology advanced and the numb er of
sensors can b e reduced with the adoption of Convolutional Neural Networks (CNNs)</p>

<p>provided by raw came ra inputs.
Increasing the use of deep learning, it allows to deve lop ers to teach to vehicle the
system to accomplish and the control achieve s numerous b enets such as the ability
to adapt to new scenarios [7], [8].
Figure 14: Autonomous Driving Sensors
4.1 Sensors
Sensors are de vic es, mo dule, machine or subsystem that pro duce an output signal
for the aim of sensing events or changes in the environment. They are vital b ecause
26</p>

<p>they grant to autonomous vehicles to plan their routes securely, sup ervise their sur-
roundings and identify oncoming imp ediments. Thanks to sensors, the automation</p>

<p>system c ombines automotive software and hardware and it takes full control of the</p>

<p>vehicle [9]. Sensors are divide d into:
â€¹
Internal state, or proprio ceptive sensors: they detect internal data wheel load,</p>

<p>angular rate, force and stores the dynamical state of a dynamic system. Some</p>

<p>examples of internal states are: enco ders, Inertial Measurement Units (IMU),</p>

<p>gyroscop es.
â€¹
Extero ceptive sensors, or external state se ns ors : they receive and collect infor-</p>

<p>mation from the system environment. For instance , they p erceive information</p>

<p>ab out light inte ns ity or distance measureme nts. An example of extero ceptive</p>

<p>sensors are cameras, LiDar, ultrasonic sensors, radar.
They can have two kind of nature:
â€¹
Passive : they obtain energy from environment and they provide the corre-</p>

<p>sp onding output. An example of passive senors are the vision cameras.
â€¹
Active: they release energy into the environment and they detect the correl-</p>

<p>ative environmental feedback. An example of active se nors are LiDAR and
radar sensors.
They are also split according to the wireless technology transmission range: short-
range, medium-range and long-range.
4.2 Detailed study ab out sensors
This section analyzes b enets and drawbacks of a generic sensor that c an b e used
in autonomous vehicle and it go es in deep into the ones used for the Bosch Future</p>

<p>Mobility challenge.
4.2.1 Camera
In autonomous vehicle, cameras are the most used te chnology to analyze the sur-
rounding. They generate images of the approaching environment, such as a p edes-</p>

<p>trian crosswalk, and they can op erate in di
erent weather condition. They can b e
classied as:
â€¹
Visible cameras (VIS): s im ilar to human eyes, they capture wavelengths that
ranges from 400 to 780 nm [10]. Combination of more visible c ameras let s tereo
vision to b e p e rformed. They are used for their ability to distinguish colors,
high resolution and low cost in spite of their low e stimated depth accuracy.
â€¹
Infrared cameras (IR): they work with infrared wavelengths ranging b etween
780 nm and 1 mm and they can b e extended to the near-infrared (NIR: 780
nm{3 mm) and the mid-infrared (MIR:3{50 m m; known as thermal cameras)</p>

<p>[10]. They are less susceptible to lighting or to weather c onditions than visible</p>

<p>camera so they can overcome situation where VIS fails and they can detect</p>

<p>warm b o dies as p edestrians.
27</p>

<p>Bosch gives to the team the Raspb erry camera b oard: Pi Camera mo dule v2.1
(Fig. 15). It is a high quality 8 megapixel camera provided with xed fo cus lens. It
is c apable of 3280 x 2464 pixel static images and it s upp orts 1080p30, 720p60 and</p>

<p>640x480p90 video us ing the Sony IMX219PQ image sensor.
Team used camera for lane keeping, intersection detection, trac sign detection,
trac light detection and ob ject detection.
Figure 15: Pi Camera mo dule v2.1
4.2.2 LiDar
The rst attempt of the light detec tion and rangig (LiDar) was develop ed in the
1930s to measure air density proles in the upp er atmosphere by dening the scat-</p>

<p>tering intensity from searchlight b eams. In 1938, for the rst time, pulses of light</p>

<p>were utilised to calculate cloud base heights. In 1953, the acronym LiDar was in-</p>

<p>tro duced by Middleton and Spilhaus. The mo dern LiDar technology was b orn with
the invention of the laser in 1960. The rst commercial LiDar had 2000 to 25,000
pulses p er sec ond for top ographic mapping applications [11].
Lidar is a distant sensing te chnique that targets a surface or an ob ject with a
laser light puls es with lengths of a few to several hundred nanoseconds and particular</p>

<p>sp ectral prop erties. The equipment measures the time b etwe en emission and recep-
tion of the light pulses p e rm iting distance estimate. It has airb orne, terrestrial and
mobile applications. The p oint cloud data (PCD) are the data that LiDar pro duces</p>

<p>and they give ob ject intensity information. Several systems make use of a b eam</p>

<p>expander to decrease the divergence of the light b eam. LiDars use mirror telescop e</p>

<p>at the rece iver e nd with lenses that can b e used for small-ap erture receivers. Emit-
ter and receiver optics can have di
erent geometric arrangement that determine the
degree of signal compression. With short distance, only a part of the LiDar return</p>

<p>signal is measured. Changing distance, b eam diameter, shap e, divergences this part</p>

<p>changes.
There are several kind of LiDar, some of them are explained b e low:
â€¹
Elastic-backscatter LiDar: in its more manageable form it applies one laser re-</p>

<p>leasing a single wavelength and one detector calculating the radiation 
e xibly
28</p>

<p>backscattered from the atmospheric particles. For example, it gives informa-
tion ab out cloud layers.
â€¹
Di
erential-absorption LiDar (DIAL): it detec ts atmospheric gases with high</p>

<p>sensitivity. It uses single broad absorption bands or absorption lines of gases.
Di
erential-absorption LiDar generates two wavelengths where one is absorb ed
more p owerfully than the other to determine the di
erential molecular absorp-</p>

<p>tion co ecient. The numb er concentration of the gas atoms can b e evaluated</p>

<p>if the di
erential absorption cross section is known. For instance, DIAL is</p>

<p>used for the observation of water vap or. In general, di
erential-absorption
LiDar must consider the Doppler broadening of the backscattered light and
it requires sp ectral purity of the emitted laser light and high stability. DIAL</p>

<p>uses the temp erature-dep endent strength of absorption lines of oxygen for tem-</p>

<p>p erature proling where the di
erential absorption cross section is me asure d</p>

<p>knowing the numb er concentration of the gas. If more than just a few nanome-
ters sp ectrally separate the two DIAL wavelengths, di
erential backscattering
is transformed into the bigger error source.
â€¹
Doppler LiDar: Direct-detec tion Doppler LiDars apply narrow-band s p ectral
lters to e valuate frequency shift and it exploits the molecular backscatter
comp onent. Coherent Doppler LiDar detects the radiation backscattered from</p>

<p>the moving particles and it emits single-mo de single -fre que nc y laser radiation.</p>

<p>A lo cal oscillator radiation is mixed generating the return s ignal for the sensor.</p>

<p>Hetero dyne detection is used to dete rm ine the sign of the shift.
In autonomous vehicle, LiDar is generally based on TOF, a pulsed laser that emits</p>

<p>pulse singularly or c ontinuously to the target triggering instantaneously internal</p>

<p>timing circuit. To obtain the distance of the ob ject, the calculator evaluates the time</p>

<p>t b etween the laser pulse getting to the target and coming back to the receiver
from the ob jective. Once the target is aimed, the laser emits light; at the same
time, the emitting signal collected by the sampler allows the counter to counting</p>

<p>and the clo ck osc illator loads the clo ck pulse to the counter. The echo signal gets</p>

<p>into the receiving optical system, it is amplied by the amplier, the photo electric</p>

<p>detector converts it into electric puls e and the counter stops counting. The c lo ck
pulse numb er entering the counter is calc ulated to get the target distance [12].
The challenge allows hardware improvement, team Politron decided to add a
TF-Luna LiDAR Mo dule. It measures the distance and it regularly releases near
infrared mo dulated waves using Time of Flight (ToF) principle (Fig 16). In order</p>

<p>to pro cure the relative distance
D
:</p>

<h1>D</h1>

<p>c
2
1
2
Ë‡ f

Ëš
(1)
where f is the clo ck pulse frequency and c is the sp eed of light, time is measured
by evaluating the phase di
erence 
Ëš
b etween original and re
ection waves.
29</p>

<p>Figure 16: Schematic of ToF princ iple
In the comp etition, LiDar was mounted on the front of the car (Fig. 17), it
detected the in-front obstacles like p edestrians and other vehicles.
Figure 17: LiDar mounted on the car of the team.
The algorithm p erform a data fusion in
MovCarProcess
b etween
ObjectDetec-
tionProcess
, the pro cess that detects the ob jects in the environment and LiDar data.
The team p erformed a sort of class ication of the detected ob jects also according to
the distance p e rceived by the LiDar: for example, p edestrians are detected with 20
cm distance, so whenever the LiDar detects an ob ject within 20 cm, it s tops since it</p>

<p>is a p edes trian, whereas other cars on track are detecte d within 75 c m distance, since</p>

<p>the car has to consider a safe margin in order to p erform the overtake mano euvre.
In the following co de, it is shown what was explained b efore:
30</p>

<p>def
 <em>lidarMeasure(self,):
try
:
ser = serial.Serial(
"
/
dev
/
serial0
"
, 115200, timeout=0)
 #
 mini
UART
 serial
 device
if
 ser.isOpen() == False:
ser.
open
()
 #
 open
 serial
 port
 if
 not
 open
distance, strength, temperature = self.read</em>tfluna<em>data(ser)
if
 distance &lt; 20.0:
 #
cm
flag</em>distance = 99
elif
 distance &gt;= 20.0
 and
 distance &lt; 30.0:
 #
cm
flag<em>distance = 2
elif
 distance &gt;= 30.0
 and
 distance &lt; 75.0:
 #
cm
flag</em>distance = 1
else
:
flag<em>distance = 0
ser.close()
return
 flag</em>distance
except
 Exception as e:
print
(
'
Lidar
 communication
 error
'
)</p>

<p>print
(e)
def
 read<em>tfluna</em>data(self, ser):
while
 True:
counter = ser.in<em>waiting
 #
 count
 the
 number
 of
 bytes
 of
 the
serial
 port
if
 counter &gt; 8:
bytes</em>serial = ser.read(9)
 #
 read
 9
 bytes</p>

<p>ser.reset<em>input</em>buffer()
 #
 reset
 buffer
if
 bytes<em>serial[0] == 0x59
 and
 bytes</em>serial[1] == 0x59:
 #
check
 first
 two
 bytes
distance = bytes<em>serial[2] + bytes</em>serial[3] * 256
 #
distance
 in
 next
 two
 bytes
strength = bytes<em>serial[4] + bytes</em>serial[5] * 256
 #
signal
 strength
 in
 next
 two
 bytes
temperature = bytes<em>serial[6] + bytes</em>serial[7] * 256
 #
temp
 in
 next
 two
 bytes
temperature = (temperature / 8.0) - 256.0
 #
 temp
 scaling
and
 offset
return
 distance, strength, temperature
4.2.3 Ultrasonic
Ultrasonic sensor or sonar is an electronic device comp osed by two main comp onents:
1.
 transmitter: it uses piezo electric crystals to emit sound.
2.
 receiver: it acquires the re
ected sound.
Ultrasonic sensors make use of echolo cation to determine the proximity of an ob ject
in the range of the sensor. It measures also the distance of the ob ject evaluating
31</p>

<p>the time for the emitted signal to come back. It emits ultrasonic s ound waves and
transforms the re 
e cted sound into an electrical signal. They lose accuracy due to</p>

<p>noise interference or its blind zone at close proximity. Ultrasonic sensors can b e</p>

<p>divided into:
â€¹
Ultrasonic proximity s ensors: they emit and receive sound waves at high fre-</p>

<p>quency. They are comp ose d by a sonic transducer which allows for alternate
transmission and reception of sound waves. They sense the pres ence of any
ob ject, re gardles s of its material or surface prop erties. They are used for</p>

<p>intermediate distances ob ject detection and they can work in bad op erating</p>

<p>conditions.
â€¹
Di
use or Re
ective sensors: when they detect an ob ject, the ultrasonic waves</p>

<p>comes back to the sensor. Any sound re
ecting, stationary ob ject is used
as a re
ector. The ultrasonic is in not active s tate as long as the measured
propagation time matches to the distance from the sensor to the re
ector. The</p>

<p>device is in active state when an ob ject comes within the sensing distance and</p>

<p>the propagation time changes. They have the transmitter and re ceiver b ox in</p>

<p>the same housing [13].
For automotive application, ultrasonic sensors transmit sonic waves in the range
of 40 kHz to 70 kHz, a range out of the audible one for humans which do es not</p>

<p>hurt human ears. This is imp ortant b ecause, for example, parking sensors of the car</p>

<p>can pro duce more than 100 dB of sound pressure [13]. The ma jority of ultrasonic</p>

<p>sensors are based on ToF principle, alre ady explained in the s ection dedicated to</p>

<p>LiDar sensors.
A second hardware improvement for the team was to add an HC-SR04 Ultrasonic
Sensor Mo dule. It utilizes sonar to determine the distance to an ob ject trough the</p>

<p>following formula:
D
= ((
S
)

time</p>

<h1>)</h1>

<p>2
where D=Distance to an ob ject, S=sp eed of sound in the air and time=time b etween</p>

<p>the transmission and reception of the signal
This s ensor reads from 2cm to 400cm (0.8inch to 157inch) with an accuracy of
0.3cm (0.1inches). It is comp os ed by an ultrasonic transmitter, which emits a high-</p>

<p>frequency sound (40 kHz), and receiver, which receives the re
ected sound (echo),</p>

<p>mo dules and it has the following sensor features [14]:
â€¹
Power Supply :+5V DC
â€¹
Quiescent Current :
&lt;
2mA
â€¹
Working Current: 15mA
â€¹
E
ectual Angle:
&lt;
15
Â°
â€¹
Ranging Distance : 2cm { 400 cm
â€¹
Resolution : 0.3 cm
â€¹
Measuring Angle: 30 degree
â€¹
Trigger Input Pulse width: 10uS TTL pulse
â€¹
Echo Output Signal: TTL pulse prop ortional to the distance range
32</p>

<p>In comp etition, the HC-SR04 Ultrasonic Sensor Mo dule is mounted on the right
side of the car, it detects the right-hand side obstacles like a vehicle and correctly</p>

<p>p erforming the overtake and parking mano e uvre.
Figure 18: Overtake m ano euvre
The overtake mano euvre is accomplished by b oth the LiDAR and the ultrasonic
sensor. Mano euvre triggered when the followings are satised at the same time:
â€¹
Dashed line.
â€¹
Obstacle (car) in front detec te d by the LiDAR.
Then, it changes the lane and p erform the overtake che cking the lateral obstacle</p>

<p>presence, and nally ends the mano euvre when the lateral sensor returns no obstacles</p>

<p>anymore (Fig. 18). The parking mano euvre will b e analysed in the Parking chapter.
In the following co de it is shown the overtake mano euvre:
elif
 OVERTAKE
 and
 OVERTAKE<em>FLAG == True:
cnt = cnt + 1
if
 cnt == 1:
print
(
"
Starting
 OVERTAKE
 manoeuvre
"
)
if
 HIGHWAY:
self.Car</em>detected = 1
if
 self.Ultrasonic == 1:
#
 The
 car
 has
 been
 detected
 by
 the
 sensor
,
 I
'
m
overtaking
car<em>detected = car</em>detected + 1
elif
 self.Ultrasonic == 0
 and
 car<em>detected &gt; 7
 and
 not
END</em>OVERTAKE:
#
 If
 I
'
ve
 detected
 the
 car
 and
 now
 it
'
s
 gone
,
 I
 can
end
 the
 overtake
 manouvre
END_OVERTAKE = True</p>

<p>for
 x
 in
 range
(6):
if
 (vec<em>s[x] == 1):
END</em>OVERTAKE = False</p>

<p>break
#
 Starting
 overtake
:
if
 cnt &lt; 25:
valore = 2001
 #
 turning
 left
33</p>

<p>#
 Overtaking
:
elif
 cnt &gt;= 25
 and
 cnt &lt; 1000
 and
 not
 END<em>OVERTAKE:
valore = self.curveVal
 #
 overtaking
,
 going
 straight
 on
elif
 cnt &gt;= 25
 and
 cnt &lt; 1000
 and
 END</em>OVERTAKE:
cnt = 1000
 #
 I
 "
reset
"
 the
 same
 counter
 with
 a
 much
higher
 value
 to
 end
 the
 manouvre
 in
 order
 to
 use
an
 unique
 counter
valore = self.curveVal
#
 Ending
 overtake
:
elif
 cnt &gt;= 1000
 and
 cnt &lt; 1025:
valore = 2002
 #
 turning
 right
elif
 cnt &gt;= 1025:
valore = self.curveVal</p>

<p>print
(
"
OVERTAKE
 manoeuvre
 finished
"
)</p>

<p>cnt = 0</p>

<p>car_detected = 0</p>

<p>OVERTAKE = False</p>

<p>END_OVERTAKE = False
NORMAL = True
else
:
print
(
"
Overtake
 error
"
)
4.2.4 ATM103 Enco der
An enco der in digital electronics is a device that measure rotation. Connected to
appropriate electronic circuits and with appropriate mechanical connections, the en-
co der is capable of measuring angular displacements, rectilinear and circular move-</p>

<p>ments as we ll as rotational sp eeds and ac celerations. There are various techniques</p>

<p>for motion detec tion: angular capacitive, magnetic, inductive and photo electric. It</p>

<p>is often used for parking in order that car can unde rs tand when it is parallel to the</p>

<p>street and nish the mano euvre. Team Politron did not use it.
4.2.5 IMU Sensor
IMU sensor is a device to meas ure orientation, gravidational forc e and velo city.
At the b eginning, technology consisted of sensor accelerometers to evaluate the
inertial acceleration and gyroscop es to measure angular rotation. In this case, IMU</p>

<p>technology has six DOF b ecause b oth sensors have three degrees of freedom. Each</p>

<p>sensor c an measure angles and in order to obtain more accurate output, b oth data</p>

<p>can b e calibrated.
Nowadays, IMU technology progresses with the magnetometer that evaluates
the b earing magnetic dire ction improving the reading of gyroscop e. In this case,</p>

<p>IMU technology has 9 DOF b ecause also magnetometer sensor has three degrees</p>

<p>of freedom. It is used for dynamic orientation calculation in the short and long</p>

<p>run when less drift errors o ccur but in the environment fe rromagnetic metal can b e
present and the measurement could b e altered by magnetic eld disturbance s [15].
Bosch provides to teams the smart IMU se ns or BNO055 (Fig. 19). It is a
System in Package (SiP) solution that integrates a triaxial 14-bit acce le rom eter, an
accurate clos e-lo op triaxial 16-bit gyroscop e, a triaxial geomagnetic sensor and a</p>

<p>32-bit micro controller running the BSX3.0 FusionLib software. It is really small [1].
34</p>

<p>Figure 19: IMU sensor BNO055
Team Politron used IMU sensor mo dule to detect the orientation of the car and
thus to correct its p ositioning inside the lane. When the co de is run, the car must
b e p ositioned p erfe ctly straight in the lane in order to set the angle 0 of the IMU in</p>

<p>such a way that the sensor can detect the right angulation of the car resp ect to the</p>

<p>street and it can correct its p osition. Bes ide this, it is used to detect the ramp due
to the change in the inclination level.
Below an example of co de used in comp etition:
def
 IMU<em>initialization(self):
self.SETTINGS</em>FILE =
 "
RTIMULib
"
print
(
"
Using
 settings
 file
 "
 + self.SETTINGS_FILE +
 "
.
ini
"
)</p>

<p>if
 not
 os.path.exists(self.SETTINGS<em>FILE +
 "
.
ini
"
):
print
(
"
Settings
 file
 does
 not
 exist
,
 will
 be
 created
"
)
self.s = RTIMU.Settings(self.SETTINGS</em>FILE)</p>

<p>self.imu = RTIMU.RTIMU(self.s)</p>

<p>print
(
"
IMU
 Name
:
 "
 + self.imu.IMUName())</p>

<p>if
 (
not
 self.imu.IMUInit()):
print
(
"
IMU
 Init
 Failed
"
)</p>

<p>self.stop()</p>

<p>sys.exit(1)
else
:
print
(
"
IMU
 Init
 Succeeded
"
)
self.imu.setSlerpPower(0.02)
self.imu.setGyroEnable(True)
self.imu.setAccelEnable(True)</p>

<p>self.imu.setCompassEnable(True)
self.poll<em>interval = self.imu.IMUGetPollInterval()
print
(
"
Recommended
 Poll
 Interval
:
 %
dmS
\
n
"
 % self.poll</em>interval)
def
 _IMUMeasure(self,):
if
 self.imu.IMURead():
self.data = self.imu.getIMUData()</p>

<p>self.fusionPose = self.data[
"
fusionPose
"
]</p>

<p>self.accel = self.data[
"
accel
"
]
self.roll = math.degrees(self.fusionPose[0])
self.pitch = math.degrees(self.fusionPose[1])</p>

<p>self.yaw = math.degrees(self.fusionPose[2])</p>

<p>self.accelx = self.accel[0]
35</p>

<p>self.accely = self.accel[1]
self.accelz = self.accel[2]
print
(
"
roll
 =
 %
f
 pitch
 =
 %
f
 yaw
 =
 %
f
"
 % (self.roll, self.pitch,
self.yaw))
def
 <em>straight</em>correction(self, starting<em>yaw):
#
 Retreive
 the
 orientation
:
if
 starting</em>yaw &lt;= 45
 or
 starting<em>yaw &gt;= 315:
yaw</em>ref = self.yaw<em>ref</em>N
elif
 starting<em>yaw &gt;= 135
 and
 starting</em>yaw &lt;= 225:
yaw<em>ref = self.yaw</em>ref<em>S
elif
 starting</em>yaw &gt; 45
 and
 starting<em>yaw &lt; 135:
yaw</em>ref = self.yaw<em>ref</em>E
elif
 starting<em>yaw &gt; 225
 and
 starting</em>yaw &lt; 315:
yaw<em>ref = self.yaw</em>ref<em>O
#
 Keep
 the
 straight
 manoeuvre
 close
 to
 the
 reference
if
 yaw</em>ref == 0:
if
 self.Yaw &gt; 5:
valore = 15
elif
 self.Yaw &lt; 355:
valore = -15
else
:
valore = 2000
else
:
if
 self.Yaw &lt; yaw<em>ref - 5:
valore = 15
 #
turn
 right
elif
 self.Yaw &gt; yaw</em>ref + 5:
valore = -15
 #
turn
 left
else
:
valore = 2000
 #
go
 straight
return
 valore
4.3 Lo calisation
In autonomous driving, the vehicle must b e provided of sensors, actuators , com-
puters and algorithms as lo calization, planning, control and p erception are needed.</p>

<p>Lo calisation system give s the geographic p osition of the vehicle using the Global
Positioning System, dead reckoning and roadway maps [16]. For a correct function
of the autonomous op eration, planning, c ontrol and p erception sys te m need lo cation</p>

<p>from lo calization system. Lo calization system must consider also particular weather</p>

<p>and driving conditions such as obscured road, fog, etc. The global p ositioning sys-</p>

<p>tem (GPS) has multipath, low accuracy and signal blo ckage issues but it is cheap
and for this reason it is often use d to provide solutions for lo calisation. To have a
robust lo calisation system, two solution can b e considered:
1.
 Development of advanced sensors (LiDAR, camera or RADAR etc.).
2.
 Fusing sensors data with network infras tructure.
To access to various vehicle information such as trac information, close cars or
weather, the ve hic le can connect through V2V (vehicle-to-vehicle) system that is
36</p>

<p>comp osed by wireless c onnec tivity emb e dded to enhance robustness to tackle the
line-of-sight problems and lo calization accuracy [17]. The vehicle-to-vehicle tech-</p>

<p>nique takes sensor information from neighb ouring vehicles into consideration to es-</p>

<p>timate lo cation forming a network (VANET) of vehicles where each car is informed</p>

<p>ab out lo cation and movement of neighb ouring vehicles. This metho ds is very cheap</p>

<p>resp ect to LiDar measurement bas ed systems. More vehicles co op eratively calibrate
their p osition and identify nearby vehicles through their GPS receive rs and ranging
sensors. The syste m works until surrounding vehicles do not or cannot participate in</p>

<p>the system. Each car obtains s ome pieces of p ositioning information di
erent degrees</p>

<p>of accuracy and combine them to minimize estimation errors [18]. The accuracy of</p>

<p>lo calization of V2V co op erative lo calization system is determined by the numb er of
the share d p osition and vehicle connected in the surrounding area (Fig.20). Same
reasoning can b e done to receive information from infrastructure or sensor nearby</p>

<p>the car with V2I and V2S techniques.
Figure 20: V2V real life representation
4.3.1 Lo calisation and mapping for BFMC2022
The vehicle used for the comp etition has an indo or lo calization system with the
goal to detect and to transmit the relative p osition for each car mo del on the track.</p>

<p>Lo calization system has three principal comp onents:
â€¹
Server: it evaluates the p osition of the cars collecting data from camera c lie nt</p>

<p>and it sends their co ordination to the Rob ot clients.
â€¹
Rob ot: it re ceives the co ordination of cars from se rver.
â€¹
Camera client: it sends the p osition of the car to the server.
And it has the following sp ecic:
â€¹
The frequency of the given messages is 3-4 Hz.
â€¹
The error of the system is ab out
Ë˜
10 cm.
37</p>

<p>â€¹
The delay of the re ceived messages is
Ë˜
0.4 seconds.
â€¹
The last detected p osition is stored and served. If the p osition is older then 2
seconds it is discarded.
To navigate in the track, a digital map in XML format is provided. It contains
two imp ortant information: no des and connections (Fig.21). The distance b etween</p>

<p>two no des is approximately 40 cm, every no de is p ositioned in the middle of the lane</p>

<p>and it has three features: Id, X co ordinate and Y co ordinate.
Figure 21: Table of No de and Connections [1].
The relation b etween two no des is de scrib ed in the connection where the start
no de id, the end no de id and the typ e of connection (s traight or dotted road: TRUE</p>

<p>or FALSE) data are given. In the intersection case, there will b e three or four p oints
with the same co ordinates, each one for a di
erent direction [1].
In the com p etition, the networkX library is used. It is a package for the cre-
ation, manipulation and study of the structure, dynam ic s, and functions of complex
networks, including [19]:
â€¹
Data structures for graphs, digraphs, and multigraphs.
â€¹
Many s tandard graph algorithms.
â€¹
Network structure and analysis measures.
â€¹
Generators for classic graphs, random graphs , and synthetic networks.
â€¹
No des can b e "anything" (e.g., text, images, XML records).
â€¹
Edges can hold arbitrary data (e.g., weights, time- series).
â€¹
Op en source 3-c laus e BSD license.
â€¹
Well tested with over 90% co de coverage .
â€¹
Additional b enets from Python include fast prototyping, easy to teach, and</p>

<p>multi-platform.
38</p>

<p>Figure 22: Comp etition track with no des and connections [1].
To save and use the map (Fig. 22), it is useful to download it in the same folder
where the lo calisation c o des are present. The function
saveGraph()
is created to
accomplish the goal. Furthermore, to read and to pro cess the attributes of the no des
it is needed an iteration through them, extracting the data from them.
def
 _saveGraph(self):
'''
Function
 created
 to
 read
 .
XML
 file
,
 save
 node
 and
 connection
information
'''
x = []
y = []</p>

<p>line = []</p>

<p>source_node = []</p>

<p>bool<em>val = 0
#
 read
 graph
G = nx.read</em>graphml(
'
./
src
/
data
/
Competition_track
.
graphml
'
)</p>

<p>pos = nx.circular<em>layout(G)
for
 (node, node</em>pos)
 in
 pos.items():
node<em>pos[0] = G.nodes[node][
'
x
'
]
x.append(node</em>pos[0])</p>

<p>node_pos[1] = G.nodes[node][
'
y
'
]</p>

<p>y.append(node<em>pos[1])
#
 print
 graph
plt.clf()
nx.draw</em>networkx(G, pos)</p>

<p>plt.show()</p>

<p>print
(
'
\
n
'
)
#
 save
 edges
for
 n, data
 in
 G.edges.items():
if
 data[
'
dotted
'
]:
39</p>

<p>bool<em>val = 1
line.append(bool</em>val)
else
:
bool_val = 0</p>

<p>line.append(bool_val)
#
 save
 nodes</p>

<p>for
 node
 in
 G.nodes(data=True):
source<em>node.append(node[0])
return
 x, y, line, source</em>node
Moreover, there is a mo dule that gets the p osition and orientation of the car
itself on the map from the lo calization system in the following way. At rst, on the
broadcast p ort '12345', server streams its TCP p ort, client sends its own ID and it</p>

<p>connects to the serve r on the communicated TCP p ort. Afterwards , server resp ons es
with an encrypted message with its private key that client translates with the server
public key and it checks that the messages are equal. If they are, the connection</p>

<p>b etween server and client b egins and the server shares its p osition and orientation</p>

<p>on the map in order to validate the p osition on the track at each moment. Bosch</p>

<p>released a co de where the vehicle uses application programm ing interface (APIs) for
communications. API is a typ e of software interface that more computers use to
communicate with each other. It validates and connects server with the c ar given</p>

<p>ID where the server gives back the p osition of the car on the track. Lo calisation</p>

<p>system s cripts are created to intercept all data, they are created by Bosch engineers</p>

<p>[20] and they are adapted to the requirement of the pro ject:
â€¹
Lo csys.py script: it is a thread that receive s from the Tra jectory pro cess (that</p>

<p>will b e explained in the section dedicate d at the tra jectory planning) the id</p>

<p>numb er, the server public key and b eacon. GPS Tracker connects to the server
and it receives co ordinate of the car on the race. In <code>setup state', it creates
the connection with server. In</code>listening state', it receives the messages.
def
 listen(self):
"""
 Listening
 the
 coordination
 of
 car
"""
coord = self.<strong>position<em>listener.listen()
 #
 listen
 messages
from
 server
 and
 receive
 coordinates
while
 self.</strong>running:
if
 self.</em><em>server</em>data.socket != None:
try
:
self.q1.put(coord)
except
 Exception as e:
print
(
'
Errore
 in
 thread
 LOCSYS
'
)
â€¹
Server
data.py script: it is the rst function called in
locsys.py
. It needs
server connection and it includes parameters of the server that are up dated in
ServerListener
and
SubscribeToServer
classes.
â€¹
Server
listener.py script: its goal is to nd the server. It stands until a broad-
cast message that contains a p ort where the server listens the car client arrives</p>

<p>on predened proto col. It ends the listening when the mess age is correct and</p>

<p>a subscrib er ob ject tries to connect on server. It has a function
nd()
that
40</p>

<p>creates a so cket with predened parameters, where it waits the broadcast mes-
sages (a p ort numb er where the server listen the car clients that it conve rts to</p>

<p>an inte ger value).
def
 find(self):
try
:
#
:
 create
 a
 datagram
 socket
 for
 intramachine
 use
s = socket.socket(socket.AF<em>INET, socket.SOCK</em>DGRAM)
#
:
 used
 to
 associate
 socket
 with
 a
 specific
 network
 interface
and
 port
 number
s.setsockopt(socket.SOL<em>SOCKET, socket.SO</em>BROADCAST, 1)
s.setsockopt(socket.SOL<em>SOCKET, socket.SO</em>REUSEADDR, 1)
s.bind((
'
'
, self.<strong>server<em>data.beacon</em>port))
#
:
 Listen
 for
 server
 broadcast
s.settimeout(1)
while
 ((
not
 self.</strong>server<em>data.is</em>new<em>server)
 and
self.</em>_running):
try
:
#
 waiting
 for
 the
 beacon
.
#
 Receive
 data
 from
 the
 socket
.
 Buffer
 size
 =
 1500
 bytes</p>

<p>data, server<em>ip = s.recvfrom(1500, 0)
#
 convert
 the
 received
 message
subscriptionPort =
 int
(data.decode(
"
utf
-8
"
))
#
 actualize
 the
 parameter
 of
 server</em>data
 with
 new
 IP
address
 and
 communication
 port
self.<strong>server<em>data.serverip = server</em>ip[0]
self.</strong>server<em>data.carSubscriptionPort = subscriptionPort
self.<strong>server</em>data.is<em>new</em>server = True
â€¹
Server
subscrib er.py script: it has the role to subscrib e on the server, to cre-
ate a connection with the parameter of
server
data.py
and to verify the server
authentication. It sends the identication numb er of car after creating a con-
nection and the server authorizes it through two mess ages. The authentication
is based on the public key of server stored in `publickey.p em' le. It has a func-
tion
subscribe()
that connects to the server and se nd the car id. After sending
the car identication numb er it checks the server authentication.
def
 subscribe(self):
try
:
#
 creating
 and
 initializing
 the
 socket
sock = socket.socket(socket.AF<em>INET, socket.SOCK</em>STREAM)
sock.connect((self.</strong>server<em>data.serverip,
self.<strong>server</em>data.carSubscriptionPort))
sock.settimeout(2.0)
#
 sending
 car
 id
 to
 the
 server
msg =
 "
{}
"
.
format
(self.</strong>carId).encode(
'
utf
-8
'
)
41</p>

<p>sock.sendall(msg)
#
 receiving
 response
 from
 the
 server
msg = sock.recv(4096)</p>

<p>#
 receiving
 signature
 from
 the
 server</p>

<p>signature = sock.recv(4096)
#
 verifying
 server
 authentication
is<em>signature</em>correct = verify<em>data(self.</em><em>public</em>key, msg,
signature)
#
 Validate
 server</p>

<p>if
 msg ==
 '
'
 or
 signature ==
 '
'
 or
 not
is<em>signature</em>correct:
msg =
 "
Authentication
 not
 ok
"
.encode(
'
utf
-8
'
)</p>

<p>sock.sendall(msg)</p>

<p>raise
 Exception(
"
Authentication
 failed
"
)
msg =
 "
Authentication
 ok
"
.encode(
'
utf
-8
'
)
sock.sendall(msg)
self.<strong>server<em>data.socket = sock
self.</strong>server</em>data.is<em>new</em>server = False
â€¹
Position
listener.py script: it aims to re ceive all messages from the server.
After the subscription on the se rver, the function
listen()
tunes the messages
on the previously initialed so cke t, it deco des and stores it in `co or' memb er
parameter that is up date by each new message. The server transmits acquired
co ordinate and timestamp of the detected car.
def
 listen(self):
while
 self.<strong>running:
if
 self.</strong>server<em>data.socket != None:
try
:
msg = self.<strong>server</em>data.socket.recv(4096)
msg = msg.decode(
'
utf
-8
'
)
if
(msg ==
 '
'
):
print
(
'
Invalid
 message
.
 Connection
 can
 be
interrupted
.
'
)
break
coor = json.loads((msg),cls=ComplexDecoder)
self.</strong>streamP<em>pipe.put(coor)
self.</em><em>server</em>data.is<em>new</em>server = False</p>

<p>self.<em>_server</em>data.socket = None</p>

<p>self.<em>_server</em>data.serverip = None
4.3.2 V2X for BFMC2022
At the comp etition, teams shared the MAC of the car c ompute r b ecause all the cars
must connect to the LAN of Bosch in order to have V2X communication.
42</p>

<p>â€¹
Semaphores
Connecting to the LAN, cars receive each semaphore broadcast messages with
a frequency of 10 Hz, including semaphore ID and its state:
0
1
2
RED
YELLOW
GREEN
Table 1: Semaphore State [1]
In Bos ch lo cation, trac lights stream, using UDP proto col, their p osition
on the network in the 5007 p ort and the y also broadcast their state. The
thread called
traclights.py
gets all the data and saves it as its attributes. As
long as the thread runs, for each semaphore ID received from the so cket the
corresp onding state (color) is asso ciated. On the rac e track, four semaphore</p>

<p>were present: one at the starting p oint and the other three placed in the</p>

<p>intersections. Team Politron creates a pro cess called
TracStateProcess
that
uses the
traclights.py
thread, in fact it imp orts the le to rec eive the state
of 4 semaphores on the track. It denes the p osition of each s emaphore a</p>

<p>priori known and the color lists as red, yellow and green. It receives the team</p>

<p>car co ordinate from lo calisation and if the car is ne ar to a sp ecic semaphore</p>

<p>it sends its state. Below,
runListener
function in the
TracStateProcess
is
shown:
def
 runListener(self,inPs,outPs):
try
:
#
 Semaphore
 colors
 list
colors = [
'
red
'
,
'
yellow
'
,
'
green
'
]
#
 Create
 listener
 object
Semaphores = trafficlights.trafficlights()
#
 Start
 the
 listener
Semaphores.start()
while
 True:
#
 Receive
 car
 coordinates
coora = inPs[0].recv()
#
 Save
 car
 coordinates
 (
KitKat
 is
 the
 car
 name
)
x<em>KitKat = coora[
'
coor
'
][0].real
y</em>KitKat = coora[
'
coor
'
][0].imag
#
 Sempahore
 position
 on
 the
 track
x_semaforo1 = 3.05</p>

<p>y<em>semaforo1 = 11.41
x</em>semaforo2 = 2.1
y_semaforo2 = 10.83</p>

<p>x_semaforo3 = 0.82</p>

<p>y_semaforo3 = 14.29</p>

<p>x_semaforo4 = 3.63</p>

<p>y_semaforo4 = 10.4
43</p>

<p>#
 Sending
 the
 state
 for
 each
 semaphore
if
 abs
(x<em>KitKat-x</em>semaforo1) &lt;= 0.4
 and
abs
(y<em>KitKat-y</em>semaforo1) &lt;= 0.4:
self.SemaphoreState = Semaphores.q1.get()
elif
 abs
(x<em>KitKat-x</em>semaforo2) &lt;= 0.4
 and
abs
(y<em>KitKat-y</em>semaforo2) &lt;= 0.4:
self.SemaphoreState = Semaphores.q2.get()
elif
 abs
(x<em>KitKat-x</em>semaforo3) &lt;= 0.4
 and
abs
(y<em>KitKat-y</em>semaforo3) &lt;= 0.4:
self.SemaphoreState = Semaphores.q3.get()
elif
 abs
(x<em>KitKat-x</em>semaforo4) &lt;= 0.4
 and
abs
(y<em>KitKat-y</em>semaforo4) &lt;= 0.4:
self.SemaphoreState = Semaphores.q4.get()
else
:
self.SemaphoreState = 100
outPs[0].send(self.SemaphoreState)
#
 Stop
 the
 listener
Semaphores.stop()
â€¹
Environmental server
At the lo cation, cars sent to lo cal environmental server at TCP p ort "23456"
the p osition and the id of the encountered obstacle (as shown in table 4.3.2).
In order to connect to the s erver, cars sent its own ID and the ID crypte d</p>

<p>with the car private key in its turn dec rypted by server with corresp onding</p>

<p>public key. If the ID's corresp onded, the client was validated by the server.</p>

<p>Consequently, serve r replied with encrypted message s with own private key
that client decrypted with the server public ke y and it che cked if the message
were the same. If it were, connection was initiated.
ID
Obstacle
1
TS - Stop
2
TS - Priority
3
TS - Parking
4
TS - Crosswalk
5
TS - Highway entrance
6
TS - Highway exit
7
TS - Round Ab out
8
TS - One way road
9
Trac light
10
Static car on road
11
Static car on parking
12
Pedestrian on crosswalk
13
Pedestrian on road
14
Roadblo ck
15
Bumpy road
Table 2: ID assignment for each obstacle [1]
44</p>

<p>As in lo calization, environmental mo dule is comp os ed by several scripts in
order to reach its goal. Firstly,
environmental.py
script is a thread that con-
nects on the server and sends messages, which incorp orates the co ordinate of</p>

<p>the e ncountered obstacles on the racetrack. It creates the connection with</p>

<p>server in the setup state, moreover in streaming state it sends the messages</p>

<p>to the serve r. Then
serverdata.py
script is a class that contains parameters
up dates in
ServerListener
and
SubscribeToServer
classes. The former class
goal is to nd the server. It stands until a broadcast message that contains a</p>

<p>p ort where the server listens the car client arrives on predened proto c ol. It</p>

<p>ends the listening when the message is correct and a s ubsc rib er ob ject tries to</p>

<p>connect on server. It has a function
nd()
that creates a so cket with prede-
ned parameters, where it waits the broadcast messages (a p ort numb er where</p>

<p>the server listen the car clients that it converts to an integer value).
Subscri-
beToServer
class has the role to subscrib e on the server, to create a connection
with the parameter of
server
data.py
and to verify the server authentication.
It sends the identication numb er of car after creating a connection and the
server authoriz es it through two messages. The authentication is based on
the public key of server stored in `publickey.p em' le. It has a function
sub-
scribe()
that connects to the serve r and send the car id. After sending the
car identication numb er it checks the server authentication. Instead,
envi-
ronmental
streamer.py
aims to send all mess age to the server.
The author cre ate d a pro cess called
EnviromentalProcess
that receives car co-
ordinates from lo calisation system and send ob jec t co ordinate (
enviromental)
to the server. It denes a vector</p>

<p>ag in order not to send several times co or-
dinates and ID required by the comp etition, once is sucient. It acquires sign</p>

<p>ag when in
TracSignProcess
a sign is detected, it obtains ob ject 
ag when
ObjectDetectionProcess
senses a sp ecic ob ject. Sign 
ag and ob jec t 
ag is
equal to a sp ecic numb e r according to the sign or ob ject that the car detects.
Making the use of
environmental.py
, it connects to the server and send ID and
co ordinates of the obstacle encountered. Below,
environmental
function of
the
EnviromentalProcess
is shown:
def
 <em>environmental(self, inPs,gpsStS</em>env):
try
:
vector_flag=[0,0,0,0,0,0,0,0,0,0,0,0,0,0]
while
 True:
try
:
#
Receive
 from
 localisation
 car
 coordinate</p>

<p>coora = inPs[0].recv()</p>

<p>#
 Save
 coordinate</p>

<p>x_gps = coora[
'
coor
'
][0].real</p>

<p>y_gps = coora[
'
coor
'
][0].imag</p>

<p>#
 Receives
 sign
 from
 image
 processing
sign = inPs[1].recv()
#
 Receives
 object
 from
 object
 detection</p>

<p>object_ = inPs[2].recv()</p>

<p>#
 Dynamic
 car
 in
 nearby</p>

<p>car<em>in</em>m = inPs[3].recv()
if
 sign == 1
 and
 vector<em>flag[0] == 0:
#
STOP
vector</em>flag[0] = vector_flag[0] +1
45</p>

<p>a = {
'
obstacle<em>id
'
: sign,
 "
x
"
: x</em>gps,
"
y
"
:y<em>gps}
gpsStS</em>env.send(a)
if
 sign == 111
 and
 vector<em>flag[0] == 0:
#
STOP
vector</em>flag[13] = vector<em>flag[13] +1
a = {
'
obstacle</em>id
'
: 1,
 "
x
"
: x<em>gps,
"
y
"
:y</em>gps}
gpsStS<em>env.send(a)
if
 sign == 2
 and
 vector</em>flag[1] == 0:
#
PRIORIY</p>

<p>vector<em>flag[1] = vector</em>flag[1] +1</p>

<p>a = {
'
obstacle<em>id
'
: sign,
 "
x
"
: x</em>gps,
"
y
"
:y<em>gps}
gpsStS</em>env.send(a)
if
 sign == 3
 and
 vector_flag[2] == 0:
#
PARKING</p>

<p>vector<em>flag[2] = vector</em>flag[2] +1</p>

<p>a = {
'
obstacle<em>id
'
: sign,
 "
x
"
: x</em>gps,
"
y
"
:y<em>gps}
gpsStS</em>env.send(a)
if
 sign == 4
 and
 vector_flag[3] == 0:
#
CROSSWALK</p>

<p>vector<em>flag[3] = vector</em>flag[3] +1</p>

<p>a = {
'
obstacle<em>id
'
: sign,
 "
x
"
: x</em>gps,
"
y
"
:y<em>gps}
gpsStS</em>env.send(a)
â€¹
V2V
The car of each team receives ID and p osition of dynamic obstacle through Wi-
Fi UDP messages with 4 Hz of frequency and 10 cm radius of accuracy. The</p>

<p>thread called
vehicletovehicle.py
gets all the data and saves it as its attributes.
As long as the thread runs, it receives indo or p ositioning and orie ntation of</p>

<p>the moving obstacle/car that are streaming their p osition into the network.</p>

<p>The author wrote a pro cess called
V2VProcess
that receives from serve r the
co ordinates of a car di
erent from the car of the team and it receives team car</p>

<p>co ordinates from lo calisation system. If they are near to each other, it sends
the other car co ordinates to
MovCarProcess
.
MovCarProcess
is the pro cess
that sends commands to the Nucleo b oard.
V2VProcess
was not used in the
comp etition b ecause there was to o much pro cess in parallel and the c ar had
problem to c ompute all the task at the same time. Below,
runListenerV2V
function of the V2VPro cess is shown:
def
 <em>runListenerV2V(self,inPs,outPs):
try
:
start</em>time = time.time()
vehicle = vehicletovehicle.vehicletovehicle()</p>

<p>#
 Start
 the
 listener</p>

<p>vehicle.start()
46</p>

<p>while
 True:
#
 Receives
 team
 car
 coordinate
coora = inPs[0].recv()</p>

<p>x_KitKat = coora[
'
coor
'
][0].real</p>

<p>y_KitKat = coora[
'
coor
'
][0].imag</p>

<p>#
Receive
 the
 other
 car
 coordinates
self.coorV2V = vehicle.pos
x_V2V = self.coorV2V[
'
coor
'
][0].real</p>

<p>y_V2V = self.coorV2V[
'
coor
'
][0].imag</p>

<p>#
 Sending
 V2V
 position</p>

<p>if
 math.sqrt(math.
pow
((x<em>V2V - x</em>KitKat),2)
+ math.
pow
((y<em>V2V - y</em>KitKat),2)) &gt;= 0
and
 math.sqrt(math.
pow
((x<em>V2V -
x</em>KitKat),2) + math.
pow
((y_V2V -</p>

<p>y_KitKat),2)) &lt;= 1:
outPs[0].send(self.coorV2V)
else
:
NO=0</p>

<p>outPs[0].send(NO)
47</p>

<p>5 Tra jectory and path planning
5.1 Tra jectory planning
An autonomous vehicle must move from a starting p oint to a nal one. To do
that, a predened path according to the vehicle or path limits is needed. Cars have</p>

<p>limited mane uvers due to limited steering angle or inaccessibility of certain places,</p>

<p>such as smaller streets or streets with work in progress. The rst step is to plan
a tra jectory that designs a predened path, that starts from p oint A and e nds in
p oint B, according to a dened time law. It is imp ortant to underline the di
erence</p>

<p>b etween path and tra jectory: the former is the place of p oints that the vehicle has</p>

<p>to follow to reach the end p oint (a purely geometric meaning), the latter is a path</p>

<p>in which a timing law is sp ecied [21]. Tra jec tory planning (Fig.23) consists in
path planning and movement planning based on ve lo city, tim e and kinematics. A
tra jec tory planner is a to ol that computes a set of reference values, inputs of the</p>

<p>control blo ck, useful to bring the autonomous plant to the desired conguration</p>

<p>given the desired kinematic, dynam ic constraints and the path. Tra jectory planner</p>

<p>must react quickly to an environment change, it must plan a feasible motion for the
autonomous vehicle, it has to control all kind of car b ehaviour [22].
Figure 23: Example of tra jectories in autonomous vehicle [2].
5.1.1 Tra jectory pl anning used in comp eti tion
The pro cess that deals with tra jectory planning is called
RaceTrajectoryProcess
.
Its principal function is called
localisation()
and its rst task is to s aves the graph
calling the function
saveGraph()
. As explained in lo calisation se ction, the team car,
in order to understand its p osition, saves the graph that represents the comp etition
track. Furthemore, it receives the co ordinates of the car from lo calisation system and
the function
understandPosition()
rep orts the exact p osition of the car in the track,
according to the GPS uncertainties (at most 10 cm). Subse quently, the pro cess calls
the function
behaviour()
, to instruct the car on which op eration it has to e xe cute.
Finally, it stores the p osition of the trac signs, of which the team s is already aware,</p>

<p>and it s ends the car p osition ne ar to the stored trac sign to
EnvironmentalProcess
.
â€¹
localisation
script is presented b elow:
def
 <em>localisation(self, inPs, q1, outPs):
try
:
#
 localisation
 of
 the
 car
 in
 the
 track
self.x, self.y, self.line, self.source</em>node =
self._saveGraph()
while
 True:
48</p>

<p>stamps= inPs[0].recv()
#
 car
'
s
 coordinate</p>

<p>coora = q1.get()</p>

<p>x_gps = coora[
'
coor
'
][0].real</p>

<p>y_gps = coora[
'
coor
'
][0].imag</p>

<p>#
car
 understands
 its
 position
 on
 the
 track
self.<em>understandPosition(x</em>gps, y_gps)
#
car
 chooses
 its
 behaviour</p>

<p>self._behaviour()</p>

<p>#
Sign
 on
 the
 track</p>

<p>self._thereIsASign()</p>

<p>outPs[0].send(self.behaviour)
 #
 send
 behaviour
 to
movecar
outPs[1].send(self.sign)
 #
send
 sign
 already
 saved
 on
the
 map
 to
 MovCarProcess
outPs[2].send(self.sign)
 #
send
 sign
 already
 saved
 on
the
 map
 to
 EnvironmentalProcess
outPs[3].send(coora)
outPs[4].send(coora)
â€¹
The
understandPosition()
function receives the co ordinates of the car and
compares them with the co ordinates of the no des present in the graph. In
order to determine the nearest no de the calculation of the euclidean distance</p>

<p>from each of them is evaluated:</p>

<h1>distance</h1>

<p>p
(
x
n
x
c
)
2
+ (
y
n
y
c
)
2
and the smallest one is selected. In the formula,
x
c
and
y
c
are the co ordinates
of the car instead
x
n
and
y
n
the co ordinate of the no de . No des has no a
precise numerical order in the graph, thus team decided to iterate, each time,</p>

<p>to all the no de present in the graph to b e sure to select the right one. In the
comp etition, the car had no problem to understand its p osition and it was
relatively fas t to fulll its goal.
def
 <em>understandPosition(self, x</em>gps, y<em>gps):
'''
This
 function
 verifies
 that
 the
 car
 coordinates
 coming
 from
server
 correspond
 to
 a
 position
 in
 the
 XML
 file
'''
cnt = 0
dist</em>min = 10000
ID<em>min = 0
x</em>min=0</p>

<p>y_min=0</p>

<p>for
 cnt, val
 in
 enumerate
(self.x):
#
 the
 statement
 if
 is
 used
 to
 find
 the
 corresponding
 node
of
 the
 car
 position
 with
 some
 uncertainty
dist = math.sqrt((x<em>gps-self.x[cnt])<strong>2 + (y</em>gps -
self.y[cnt])</strong>2)
if
 dist &lt; dist<em>min:
dist</em>min = dist</p>

<p>x<em>min = self.x[cnt]
y</em>min= self.y[cnt]
ID<em>min = self.source</em>node[cnt]
self.current<em>node = ID</em>min
49</p>

<p>self.current_line = self.line[cnt]
â€¹
The
behaviour()
function is the fo cal function for the tra jectory planning.
Knowing the map and the path (Fig.24) selected for the comp etition, a b e-
havior that the car has to execute is set at each no de; four vectors formed by</p>

<p>no des that help to understand the b ehaviour are established.
Figure 24: Comp etition track with the designed path.
The team selected a path to follow in order that the car accomplish the required</p>

<p>tasks. If the car is in corresp ondence of a ce rtain no de, the script sends the</p>

<p>b ehaviour to
MovCarProcess
. For instance, if the car is in the no de numb er
'68', it turns right b ecause this no de is in the vector that establishes that the</p>

<p>car must turn right.
def
 <em>behaviour(self):
'''
 According
 to
 the
 position
 it
 send
 '
go
 straight
',
 '
right
',
'
left
'
 or
 '
normal
'
 state
.
 From
 1-30
 the
 car
 is
 in
 the
intersection
,
 otherwise
 no
 start
 at
 node
 39,
 end
 at
 111
'''
self.current</em>node =
 int
(self.current_node)</p>

<p>list_gostraight=[77,32,63,72,81]</p>

<p>list_right=[68,16,25]</p>

<p>list<em>left=[2,62]
list</em>final = [131,132,133,126,125,124]
self.behaviour=0
 #
normal
 state
:
 lane
 keeping
for
 i
 in
 list<em>gostraight:
if
 self.current</em>node==i:
#
 go
 straight
self.behaviour = 2
for
 j
 in
 list_right:
50</p>

<p>if
 self.current<em>node ==j:
#
 turn
 right
self.behaviour = 4
for
 z
 in
 list</em>left:
if
 self.current<em>node == z:
#
 turn
 left
self.behaviour = 3
for
 l
 in
 list</em>final:
if
 self.current<em>node == l:
#
 end
self.behaviour = 5
if
 self.behaviour == 0
 and
 ((313 &lt; self.current</em>node
 and
self.current<em>node &lt; 336)
 or
 (375 &lt; self.current</em>node
 and</p>

<p>self.current<em>node &lt; 398)
 or
 (256 &lt; self.current</em>node
 and</p>

<p>self.current<em>node &lt; 263)
 or
 (201 &lt; self.current</em>node
 and</p>

<p>self.current<em>node &lt; 208)):
#
 NORMAL</em>DASHED</p>

<p>self.behaviour = 1
â€¹
The
thereIsASign()
function saves the already known p osition of the signs,
then this information will b e sent to
MovCarProcess
and to
Environmantal-
Process
.
def
 <em>thereIsASign(self):
self.sign= -1
node</em>STOP = [90,91,54,53, 94, 95, 68, 67]
node<em>PRIORITY = [196,197,63,62,148,149]
node</em>PARKING = [175,176,167,168]</p>

<p>node_CROSSWALK = [80,92,81,97]</p>

<p>node<em>HIGHWAY</em>ENTRANCE = [49,308,309,50,425]</p>

<p>node<em>HIGHWAY</em>EXIT = [338,339,340,345,346]</p>

<p>node<em>ROUNDABOUT = [341,342,343,344]
node</em>ONE<em>WAY</em>ROAD = [7,8]
for
 i
 in
 node<em>STOP:
if
 self.current</em>node == i:
self.sign = 1
for
 j
 in
 node<em>PRIORITY:
if
 self.current</em>node == j:
self.sign = 2
for
 k
 in
 node<em>PARKING:
if
 self.current</em>node == k:
self.sign = 3
for
 l
 in
 node<em>CROSSWALK:
if
 self.current</em>node == l:
self.sign = 4
for
 m
 in
 node<em>HIGHWAY</em>ENTRANCE:
if
 self.current<em>node == m:
self.sign = 5
for
 n
 in
 node</em>HIGHWAY<em>EXIT:
if
 self.current</em>node == n:
self.sign = 6
for
 o
 in
 node<em>ROUNDABOUT:
if
 self.current</em>node == o:
51</p>

<p>self.sign = 7
for
 p
 in
 node<em>ONE</em>WAY<em>ROAD:
if
 self.current</em>node == p:
self.sign = 8
When 'go straight', 'right', 'left' or 'normal' state is sent to
MovCarProcess
, the car
must follow sp ecic commands selected in the pro ces s:
â€¹
Go straight: the car must go straight at the intersection using IMU sensor
that can correct car p osition.
â€¹
Right: the car has to turn right at the intersection. Knowing the size and the</p>

<p>angulation of the curve, the s teering angle is set and the car turns right until</p>

<p>the IMU sensor detects 90
Â°
resp ect to the p osition in which the mano euvre
starts (Fig.25).
â€¹
Left: the car must turn left at the intersection. Knowing the size and the</p>

<p>angulation of the curve, the steering angle is set and the car turns left until</p>

<p>the IMU sensor detects 270
Â°
resp ect to the p osition in which the mano euvre
starts (Fig.25).
â€¹
Normal: the car keeps the lane.
Figure 25: Car of the team at inte rs ection with grades of IMU sensor s p ecied.
During the comp etition two challenges were a
orded: a technical one has to b e</p>

<p>executed in 10 minutes, instead the sp eed challe nge in 3 minutes. For the former,
the car had a velo city of 0.12 m/s if the steering angle was less than 25
Â°
otherwise
0.10 m/s. For the latter, the car had a velo city of 0.20 m/s if the steering angle was
less than 45
Â°
otherwise 0.10 m/s. The decision of the team was based on the ability
of the car to rec ogniz e the lane and act conse que ntly; increasing sp eed, the car had</p>

<p>diculties to act enough rapidly.
52</p>

<p>5.2 Path planning
Path planning is a fundamental matter for autonomous vehicle b ecause it has a huge
e
ect on driver and passengers safety, it is the core of the autonomous vehicle abilities
and obstacle avoidance [23]. The main goal of path planning is to nd the shortest
feasible path preventing uncertainties, taking into account vehicle dynamics, obstacle</p>

<p>maneuvering capabilities, and trac rules (Fig. 26); it is often based on a digital</p>

<p>map and it selects the route that the c ar must take according to the surrounding</p>

<p>environment. A prop er planning leads to an increase in system eciency and a</p>

<p>reduction in p ower consumption [24]. Path planning can b e global and lo cal: the
former gives the constraint to the lo cal path planning so that it can achieve an
optimal path base d on a given requirements, for example a path to reduce p ollution,</p>

<p>to decreas e the amount of time and less dangerous mano euvres. If the planned path</p>

<p>cannot b e accomplished, the global planning must relaunch to get a new fe asible</p>

<p>path. Lo cal path planning uses on-b oard sensor to unde rs tand the surroundings
information in order to lo cate the p osition of the ve hicle and obstacles on the map:
in this way it can smo othly plan the b e st path from the starting no de to target one</p>

<p>[25].
Figure 26: Example of di
erent paths on the same map.
Path Planning consists of three main steps:
1.
 It generates a sequence of mano euvres from the initial p osition to the nal
one.
2.
 It cho oses the direction to follow at e ach movement.
3.
 It has to forward planning.
There are multiple tests in planning a path for an autonomous vehicle such as
creating an oine map that represents the real world in order to help the system</p>

<p>to understand the p osition of the vehicle; planning rapidly a path through p oints of</p>

<p>the map, taking into account obstacles p osition; nding the appropriate acce le ration
and direction in order to get the more comfortable and safer path [23].
Path planning can b e classied in the following way [26]:
53</p>

<p>â€¹
Space conguration: the surrounded environmental of the vehicle is divided in
cells and, if they are collision-free, a solution is applie d. The goal is to discover</p>

<p>the right conguration of linked cells avoiding collis ion. These algorithms are</p>

<p>fast but they often give unfeasible solutions.
â€¹
Path-nders: these algorithms are based on the search of a path among no des</p>

<p>in a graph. The main purp ose is to nd the optimal path in terms of cost</p>

<p>function. Cos t function must take in consideration the p ossibility of collisions</p>

<p>and it has to take into acc ount the comfort of a path. The main path-nders</p>

<p>algorithms are: A*, Dijkstra used when the environment is previously known
and Rapidly Random Tree (RRT) employed in unknown environments.
â€¹
Attractive and repulsive forces: they creates articial forces that direct the car
to the desired destination avoiding obstacles or sp ecic areas. The sum of the
forces is the state of the motion of the vehicle. The solution of this algorithm
can b e unstable or the algorithm can blo ck into lo c al minima.
â€¹
Curves: in this algorithm, a set of parametric or/and semi-parametric curves
are generated according to the state of the car, the road and sp ecic mathe-
matical form. It can b e applied through two main metho ds: p oint-free sche me</p>

<p>and p oint-to-p oint scheme. In the former, the vehicle is able to follow a feasi-</p>

<p>ble kinematic/dynamic tra jectory with a given legal maneuve r thanks to the</p>

<p>generated curves, otherwise in the latter, curves are used to arrange the tra-
jectory b etween two p oints. Curves represent a lo cal path and it is dicult to
generate them in a correct way.
â€¹
Articial intelligence schemes: they use human reas oning to solve problems and</p>

<p>there are many techniques to nd an optimal path. Accurate mathematical
mo dels are not needed.
In this pro ject, path-nders algorithms are studied in deep. They can b e divided in
more categories [3]:
â€¹
Sampling-Based Algorithm s (Fig. 27): they are divided into active and passive</p>

<p>algorithms. The former have a pro cessing pro cedure to pro duce the b est p os-</p>

<p>sible path. LaValle [3] prop osed the Rapidly e xploring Random Tree (RRT)
metho d that is an example of active s ampling-base d: it is able to solve multi-
DOF problems and it handles path planning of di
erent kinematic constraints ;</p>

<p>RRT algorithms are based on rapidly search of the c onguration space to cre-</p>

<p>ate the path that connects the initial no de to the nal no de. The passive</p>

<p>algorithms are the combination of search algorithms to collect the b est fe asi-
ble path among all suitable paths existing in the net m ap. They make use of
the Probabilistic Road Maps (PRM) approach which is a technique that con-</p>

<p>nects some sampled p oints in the map to c reate a graph comp osed by feasible</p>

<p>routes with collision-free edges. PRM uses graph search algorithm to select</p>

<p>the optimal path.
54</p>

<p>Figure 27: Sampling based approache s with the main advantages and drawbacks [3]
â€¹
No de-Based Optimal Algorithms (Fig. 28): they nd an optimal path when in-
formation sensing and pro cessing pro cedures are already executed and they are</p>

<p>based on a principle of exploring among a set of cells in the map. No de-Based</p>

<p>Optimal algorithms are divided into two categories: grid search algorithms</p>

<p>and graph search algorithms. The former consists of mapping the environ-
ment that is divided into a set of cells in which a cell represents the presence
of an ob ject at a sp ecic p osition; if it gives evidence form information, like</p>

<p>the mass or the size value, it is called evidential o ccupancy grid. T he graph</p>

<p>search algorithms is useful to complete the rst typ e. Dijks tra algorithm is a</p>

<p>dynamic programming while A* is the heuristic search algorithms; they are
b oth improved by searching with the least cost.
55</p>

<p>Figure 28: No de-based optimal approaches with the main advantages and drawbacks
[3]
In particular, Dijkstra algorithm and A* are implemented in the following sub-
sections.
5.2.1 Dijkstra Algorithm
In 1959, Edsger Dijkstra prop osed the Dijkstra algorithm. It is a shortest path
algorithm with a simple implementation and it easily adapts to the top ology change.</p>

<p>Dijkstra algorithm uses directed graph to search the shortest path from the s ource
no de to a de stination one in a m ap with a unique source. At rst, it pro ces ses the
shortest path from source to adjacent no des, the latter is called intermediate no de.</p>

<p>Next, it rep eats the action b etween intermediate no de to its adjacent no des. It</p>

<p>nishes the iteration when every no de is traversed. It nds the shortest path from</p>

<p>source no de to any destination no de in the path b ecause it is comp osed by sub-
path that are the shortest paths from the source no de and the terminal no de. The
Dijkstra algorithm can b e considered as a kind of gree dy algorithm and considering</p>

<p>the assumptions [27]:
â€¹
G=(V,E)
is a direct graph where V=set of no des and E=set of arcs.
â€¹
n
no des of
G=(V,E)
.
â€¹
e
arcs of
G=(V,E)
.
â€¹
Dist(X)
is the distance b etween the source no de
v
to X no de.
â€¹
W
is the weight of the arc.
â€¹
S
is the set of no des inside a shortest path.
â€¹
V - S
denotes the set of no des that is not inside in any s hortes t path.
The algorithm can b e describ ed as follows (Fig.29):
1.
 It selects the source no de
v
and set S = S
[
v
.
56</p>

<p>2.
 In U =
V - S
, it nds the no de
i
that is adjacent to the source no de
v
and
has smallest weight
W
of the arc that connects the two no des. The no de
i
is
added into
S
.
3.
 Using
i
as the new interme diate no de, it rep eats the step
2
to nd the adjacent
no de
j
. If
j
can b e reached in di
erent way, for instance, from s ource no de
to no de j directly or passing through other no des, the minimum distance is
selected and stored as Dist(j).
4.
 Step
2
and step
3
are rep eated n- 1 times in order to obtain the shortest path.
Figure 29: Flowchart of Dijkstra algorithm
The author created a pro cess called
PathPlanning
to implement the Dijkstra
algorithm. It is comp osed by the main function
pathplanning()
that calls the func-
tion
saveGraph()
to save the graph of the map and it nds the shortest path. Next,
it receives the p osition of the car from lo calisation system and it calls the function
understandPosition()
to understand the p osition of the car in the track. At the
end, it us es the function
behaviourPath()
to comprehend the right b ehaviour of the
car and it sends it to
MovCarProcess
.
â€¹
pathplanning()
script is presented b elow:
def
 <em>pathplanning(self, inPs, outPs):
try
:
self.x, self.y, self.line, self.source</em>node =
self.<em>saveGraph()
while
 True:
coora = q1.get()
 #
 car
'
s
 coordinate
x</em>gps = coora[
'
coor
'
][0].real</p>

<p>y<em>gps = coora[
'
coor
'
][0].imag
self.</em>understandPosition(x<em>gps, y</em>gps)
57</p>

<p>self._behaviourPath()
outPs[0].send(self.behaviour)
 #
 send
 behaviour
 to
Movecar
except
 Exception as e:
print
(
'
Error
 in
 PathPlanning
 process
'
)
â€¹
The
saveGraph()
function is similar to the one explained in the section ded-
icated to the lo calisation and mapping for B FMC2022. It aims to save the
graph of the map. In order to read and to pro cess the attributes of the no de s</p>

<p>it is needed an ite ration and an extraction of the data from them. In this pro-</p>

<p>cess, it uses the NetworkX library, particularly
networkx.dijkstra
path
function
that uses Dijkstra's metho d to compute the shortest weighted path b etween
two no des in a graph. T he NetworkX is a Python package for the creation,
manipulation and study of the structure, dynamics, and functions of complex</p>

<p>networks [19].
def
 <em>saveGraph(self):
'''
Function
 created
 to
 read
 .
XML
 file
,
 save
 node
 and
connection
 information
'''
#
 read
 graph
G = nx.read</em>graphml(
'
./
src
/
data
/
Competition_track
.
graphml
'
)</p>

<p>pos = nx.circular<em>layout(G)
for
 (node, node</em>pos)
 in
 pos.items():
node<em>pos[0] = G.nodes[node][
'
x
'
]
x.append(node</em>pos[0])</p>

<p>node_pos[1] = G.nodes[node][
'
y
'
]</p>

<p>y.append(node<em>pos[1])
#
 print
 graph
plt.clf()
nx.draw</em>networkx(G, pos)</p>

<p>plt.show()</p>

<p>print
(
'
\
n
'
)
#
 save
 edges
for
 n, data
 in
 G.edges.items():
if
 data[
'
dotted
'
]:
bool_val = 1</p>

<p>line.append(bool<em>val)
else
:
bool</em>val = 0</p>

<p>line.append(bool_val)
#
 save
 nodes</p>

<p>for
 node
 in
 G.nodes(data=True):
source_node.append(node[0])
#
 find
 the
 shortest
 path</p>

<p>self.node<em>distance = nx.dijkstra</em>path<em>length(G,
 '
112
'
,
'
18
'
)
self.Path = nx.dijkstra</em>path(G,
 '
112
'
,
 '
18
'
)
58</p>

<p>return
 x, y, line, source_node
â€¹
The
understandPosition()
function, as in tra jectory section, receives the co-
ordinates of the car and compares them with the co ordinates of the no des
present in the graph. To determine the nearest no de the calculation of the</p>

<p>euclidean distance from each of them is evaluated and the smallest one is se-</p>

<p>lected. No des has no a precise numerical order in the graph, thus team decided</p>

<p>to iterate, each time , to all the no de pre sent in the graph to b e sure to select</p>

<p>the right one. In the comp etition, the car had no problem to understand its
p osition and it was relatively fast to fulll its goal.
â€¹
In the
behaviourPath()
function vectors with sourc e no de and target no de s
that de scrib e the p ossible b ehaviour are stored. Knowing the map, it is p oss ible
to know the feasible mano euvres from a sp ecic no de. Being aware of the path,</p>

<p>if the car is in the p osition of an "input" no de, for example "RIGHT
INPUT"
no de, and the de stination no de after the intersection is on the right and it is</p>

<p>stored in "RIGHT
OUTPUT", the b ehaviour of the car is to turn right and it
is sent to
MovCarProcess
. The
for
statement is used to iterate over the path
vector in order to understand which b ehaviour the car must follow.
def
 <em>behaviourPath(self):
RIGHT</em>INPUT = [77, 79, 43, 41, 52, 374, 50, 68, 70, 2, 4,
6, 34, 32, 59, 61, 15, 16, 25, 23]
RIGHT<em>OUTPUT = [78, 80, 44, 42, 53, 51, 51, 69, 71, 3, 5,
7, 35, 33, 60, 62, 17, 17, 26, 25]
LEFT</em>INPUT = [79, 81, 45, 43, 54, 52, 70, 72, 2, 4, 6, 34,
36, 61, 63, 25, 27]
LEFT<em>OUTPUT = [76, 78, 42, 40, 51, 49, 67, 69, 7, 1, 3,
31, 33, 58, 60, 22, 24]
STRAIGHT</em>INPUT = [77, 81, 45, 41, 54, 50, 68, 72, 2, 6, 4,
36, 32, 59, 63, 14, 18, 27, 23]
STRAIGHT<em>OUTPUT = [80, 76, 40, 44, 49, 53, 71, 67, 5, 1,
7, 31, 35, 62, 54, 17, 13, 22, 26]
self.current</em>node =
 int
(self.current<em>node)
for
 D
 in
 self.node</em>distance:
if
 D &lt; (self.node<em>distance-2):
if
 self.current</em>node==self.Path[D]:
for
 i
 in
 RIGHT<em>INPUT:
if
 self.Path[D]==RIGHT</em>INPUT(i)
 and
self.Path[D+2]==RIGHT<em>OUTPUT(i):
#
turn
 right
self.behaviour = 4
elif
 self.Path[D] == LEFT</em>INPUT(i)
 and
self.Path[D + 2] == LEFT_OUTPUT(i):
#
turn
 left</p>

<p>self.behaviour = 3
elif
 self.Path[D] == STRAIGHT<em>INPUT(i)
 and
self.Path[D + 2] == STRAIGHT</em>OUTPUT(i):
#
go
 straight
if
 ((313 &lt; self.current<em>node
 and
self.current</em>node &lt; 336)
 or
 (375 &lt;
59</p>

<p>self.current<em>node
 and
self.current</em>node &lt; 398)
 or
 (256 &lt;</p>

<p>self.current_node
 and</p>

<p>self.current_node &lt; 263)
 or
 (201 &lt;</p>

<p>self.current_node
 and</p>

<p>self.current_node &lt; 208)):
#
dotted
 lane</p>

<p>self.behaviour = 1
else
:
#
continuous
 lane</p>

<p>self.behaviour = 2
else
:
#
lane
 keeping</p>

<p>self.behaviour = 2
elif
 D==self.node<em>distance-1:
continue
elif
 D==self.node</em>distance:
if
 self.current_node == self.Path[D]:
#
stop</p>

<p>self.behaviour = 5
else
:
#
lane
 keeping
self.behaviour = 2
5.2.2 A* Algori thm
A* algorithm is an extension of Dijkstra algorithm and, in static network, it is
the most ecient approach nding the brie fe st path. A* algorithm has numerous</p>

<p>b enets, for example it has small search space or fast convergence. A* algorithm
is based on graph search metho d where a grid map represents the environment.
Traditional A* algorithm nds a path that avoids collisions resp ecting car limits and</p>

<p>constraints when the c ar structure and the p osition of obstacles are known. The path</p>

<p>in autonomous vehicle must b e comfortable, real-time and reliable, which means the</p>

<p>path planning must consider a trade-o
 b etween constraints in real time and mo del
completeness [28]. A* algorithm is a search problem therefore it is imp ortant to
set the start no de, the end no de and all the state that the c ar can bump into;</p>

<p>another signicant actions are to check the co de, take into consideration all p ossible</p>

<p>mano euvres and set movement costs (edges in the graph). It uses a function that</p>

<p>indicates the directions to follow and it nishes when the end p oint is reached.
According to the function, if a step is reasonable the algorithm p erforms it. Heuristic
metho ds are the base of the A* algorithm that nds the b est p ossible solution. A* is</p>

<p>a kind of algorithm that is guaranteed to nd a solution if the solution exist thanks</p>

<p>to the prop erty of com plete ne ss. At each no de, A* evaluates the cost
f(n)
, where
n
b eing the neighb oring no de, to move to all adjacent no des and it travels to the one
with the smallest value of
f(n)
. The
f(n)
can b e calculated in the following way:
f
(
n
) =
g
(
n
) +
h
(
n
) (2)
where
g(n)
is the value of the briefest path from the initial no de to no de
n
and
h(n)
is
the heuristic approximation of the no de value. Each no de is marked with its relative
optimal
f(n)
value in order to re construct any path. To nd the optimal solution, it
is strictly needed a correct heuristic value
h(n)
; this determines the eciency of A*.
60</p>

<p>Heuristic function have two prop erties [29]:
â€¹
Admissibility: the prop erty to not overestimate the real distance b etween
n
and the nal no de in a given heuristic function
h(n)
. For each no de, the
following formula is employed:
h
(
n
)

h

(
n
) (3)
where
h

(n)
is the real distance b etween
n
and the target no de. Nevertheless,
if the heuristic function overvalues the real distance by a value smaller than
d
, it can b e dened as a solution with accuracy equal to
d
.
â€¹
Consistency: the prop erty to e valuate an estimate s maller or equal to the</p>

<p>predicted distance b etween the nal
n
and any given adjoint adde d to the
estimated cos t to reach that sp ecic neighb our:
c
(
n; m
) +
h
(
m
)

h
(
n
) (4)
where
c(n,m)
is the distance b e tween no des
n
and
m
. The path for every no de
is optimal if
h(n)
is consistent and this denotes that the function is optimal.
The author created a pro cess called
PathPlanningAStar
to implement A* algo-
rithm. It is comp osed by the main function
pathplanning()
that calls the function
saveGraph()
to save the no des of the graph of the map and it saves the e dge s in
di
erent dictionaries. Dictionary in Python is similar to a map and it consists of</p>

<p>key-value pairs; dic tionaries are joined in
graph
create()
function and the complete
graph is stored in a xed variable in
Graph()
function. After that, the function
a
star
algorithm()
computes the A* algorithm. Next, it receives the p osition of
the car from lo calisation system and calls the function
understandPosition()
to
acknowledge the p osition of the car in the track. At the e nd, it uses the func-</p>

<p>tion
behaviourPath()
to gure out the right b ehaviour of the car and sends it to
MovCarProcess
.
â€¹
pathplanning()
script is presented b elow:
def
 <em>pathplanning(self, inPs, outPs):
try
:
self.x, self.y = self.</em>saveGraph()
#
join
 the
 lists
adjacency<em>list</em>u = self.graph<em>create(self.adjacency</em>list1,
self.adjacency<em>list</em>supp1, self.adjacency<em>list</em>supp2,
self.adjacency<em>list</em>supp3, self.adjacency<em>list</em>supp4,
self.adjacency<em>list</em>85)
#
create
 the
 graph</p>

<p>adjacency<em>list</em>u=self.Graph(adjacency<em>list</em>u)
print
(adjacency<em>list</em>u)
#
a
*
 algorithm</p>

<p>self.Path = self.a<em>star</em>algorithm(
'
77
'
,
'
127
'
)</p>

<p>while
 True:
coora = q1.get()
 #
 car
'
s
 coordinate</p>

<p>x<em>gps = coora[
'
coor
'
][0].real
y</em>gps = coora[
'
coor
'
][0].imag
self.<em>understandPosition(x</em>gps, y_gps)
61</p>

<p>self._behaviourPath()
outPs[0].send(self.behaviour)
 #
 send
 behaviour
 to
MovCarProcess
â€¹
The
saveGraph()
function reads the known graph and saves all the no des.
Edges are saved in di
erent dictionaries in order to avoid overwriting problems:
for example, if no de '2' has two destination no des as '5' and '7', the co de
subscrib es the rst with the second des tination no de , therefore only no de '2'
with destination '7' is saved.
def
 _saveGraph(self):
#
 read
 graph</p>

<p>G = nx.read<em>graphml(
'
./
Competition</em>track
.
graphml
'
)</p>

<p>pos = nx.circular<em>layout(G)
for
 (node, node</em>pos)
 in
 pos.items():
node<em>pos[0] = G.nodes[node][
'
x
'
]
x.append(node</em>pos[0])</p>

<p>node_pos[1] = G.nodes[node][
'
y
'
]</p>

<p>y.append(node_pos[1])
plt.clf()</p>

<p>nx.draw_networkx(G, pos)</p>

<p>plt.show()</p>

<p>print
(
'
\
n
'
)</p>

<p>j = 0</p>

<p>for
 n, data
 in
 G.edges.items():
j = n[0]</p>

<p>if
 j ==
 '
9
'
:
self.adjacency<em>list</em>supp1[j] = [(
'
3
'
,
 '
1
'
)]</p>

<p>self.adjacency<em>list</em>supp2[j] = [(
'
5
'
,
 '
1
'
)]</p>

<p>self.adjacency<em>list</em>supp3[j] = [(
'
7
'
,
 '
1
'
)]</p>

<p>self.adjacency<em>list</em>supp4[j] = [(
'
8
'
,
 '
1
'
)]
elif
 j ==
 '
10
'
:
self.adjacency<em>list</em>supp1[j] = [(
'
7
'
,
 '
1
'
)]</p>

<p>self.adjacency<em>list</em>supp2[j] = [(
'
5
'
,
 '
1
'
)]</p>

<p>self.adjacency<em>list</em>supp3[j] = [(
'
8
'
,
 '
1
'
)]</p>

<p>self.adjacency<em>list</em>supp4[j] = [(
'
1
'
,
 '
1
'
)]
elif
 j ==
 '
11
'
:
self.adjacency<em>list</em>supp1[j] = [(
'
7
'
,
 '
1
'
)]</p>

<p>self.adjacency<em>list</em>supp2[j] = [(
'
8
'
,
 '
1
'
)]</p>

<p>self.adjacency<em>list</em>supp3[j] = [(
'
1
'
,
 '
1
'
)]</p>

<p>self.adjacency<em>list</em>supp4[j] = [(
'
3
'
,
 '
1
'
)]
elif
 j ==
 '
12
'
:
self.adjacency<em>list</em>supp1[j] = [(
'
1
'
,
 '
1
'
)]</p>

<p>self.adjacency<em>list</em>supp2[j] = [(
'
3
'
,
 '
1
'
)]</p>

<p>self.adjacency<em>list</em>supp3[j] = [(
'
5
'
,
 '
1
'
)]
#
...</p>

<p>elif
 j ==
 '
304
'
:
self.adjacency<em>list</em>supp1[j] = [(
'
305
'
,
 '
1
'
)]
self.adjacency<em>list</em>supp2[j] = [(
'
343
'
,
 '
1
'
)]
elif
 j ==
 '
306
'
:
self.adjacency<em>list</em>supp1[j] = [(
'
231
'
,
 '
1
'
)]
self.adjacency<em>list</em>supp2[j] = [(
'
307
'
,
 '
1
'
)]
elif
 j ==
 '
468
'
:
self.adjacency<em>list</em>supp1[j] = [(
'
243
'
,
 '
1
'
)]
62</p>

<p>self.adjacency<em>list</em>supp2[j] = [(
'
468
'
,
 '
1
'
)]
elif
 j ==
 '
315
'
:
self.adjacency<em>list</em>supp1[j] = [(
'
316
'
,
 '
1
'
)]
self.adjacency<em>list</em>supp2[j] = [(
'
426
'
,
 '
1
'
)]
else
:
self.adjacency<em>list1[j] = [(n[1],
 '
1
'
)]
self.adjacency</em>list<em>85[
'
85
'
] = [(None,
 '
1
'
)]
return
 x, y
â€¹
The
graph
create()
function joins the dictionaries created in
saveGraph()
us-
ing the 'ChainMap' container, memb er of mo dule 'collection', which is com-
p osed by di
erent containers. A container is an ob ject that is employed to
store and provides the acce ss to distinct ob je cts. T he 'ChainMap' container
encapsulates se veral dictionaries into one.
def
 graph</em>create(self):
adjacency<em>list</em>t = ChainMap(self.adjacency<em>list1,
self.adjacency</em>list<em>supp1, self.adjacency</em>list<em>supp2,
self.adjacency</em>list<em>supp3, self.adjacency</em>list<em>supp4)
adjacency</em>list = {<em>*adjacency_list_t, *</em>self.adjacency<em>list</em>85}
return
 adjacency<em>list
â€¹
The
Graph()
function sets the dictionary xed in the class.
â€¹
The
a
star
algorithm()
function is pre sented b elow. Two lists are set in order
to understand if the neighb ors have b een insp ected or not: the list
g
contains
all distances among no des and the list
parents
contains an adjacency map of
all no des. A
while
statement is used to nd the lowest value of
f(n)
and, to
nd the path, it takes into consideration the rst edges of each no de; if it do es
not nd a path, it tries again with di
erent e dge s. The
if
statement is used
to understand if the path is ended. In each no de,
open
list
and
closed
list
are
up dated.
def
 a</em>star<em>algorithm(self, start</em>node, stop<em>node):
#
 open</em>list
 is
 a
 list
 of
 nodes
 which
 have
 been
 visited
,
 but
who
'
s
 neighbors
#
 haven
'
t
 all
 been
 inspected
,
 starts
 off
 with
 the
 start
 node
#
 closed<em>list
 is
 a
 list
 of
 nodes
 which
 have
 been
 visited
#
 and
 who
'
s
 neighbors
 have
 been
 inspected
open</em>list =
 set
([start<em>node])
closed</em>list =
 set
([])
#
 g
 contains
 current
 distances
 from
 start<em>node
 to
 all
 other
nodes
#
 the
 default
 value
 (
if
 it
'
s
 not
 found
 in
 the
 map
)
 is
 +
infinity
g = {}
g[start</em>node] = 0
#
 parents
 contains
 an
 adjacency
 map
 of
 all
 nodes
parents = {}
63</p>

<p>parents[start<em>node] = start</em>node
while
 len
(open_list) &gt; 0:
n = None</p>

<p>#
 find
 a
 node
 with
 the
 lowest
 value
 of
 f
()
 -
 evaluation
function
for
 v
 in
 open_list:
if
 n == None
 or
 g[v] + self.h(v) &lt; g[n] + self.h(n):
n = v
if
 n == None:
print
(
'
Path
 does
 not
 exist
!
 I
 will
 try
 again
'
)</p>

<p>adjacency<em>list =
self.self.graph</em>create(adjacency<em>list1,
adjacency</em>list<em>supp2, adjacency</em>list<em>supp1,
adjacency</em>list<em>supp3, adjacency</em>list<em>supp4,
adjacency</em>list<em>85)
path = self.Graph(adjacency</em>list)</p>

<p>self.Path = self.a<em>star</em>algorithm(start<em>node, stop</em>node)</p>

<p>return
#
 if
 the
 current
 node
 is
 the
 stop_node</p>

<p>#
 then
 we
 begin
 reconstructin
 the
 path
 from
 it
 to
 the
start<em>node
if
 n == stop</em>node:
reconst<em>path = []
self.Distanza</em>nodi=0
while
 parents[n] != n:
reconst<em>path.append(n)
self.Distanza</em>nodi=self.Distanza<em>nodi+1
n = parents[n]
reconst</em>path.append(start<em>node)
reconst</em>path.reverse()
self.Path=reconst<em>path
print
(
'
Path
 found
:
 {}
'
.
format
(reconst</em>path))
return
 reconst_path
#
 for
 all
 neighbors
 of
 the
 current
 node
 do</p>

<p>for
 (m, weight)
 in
 self.get<em>neighbors(n):
#
 if
 the
 current
 node
 isn
'
t
 in
 both
 open</em>list
 and
closed<em>list
#
 add
 it
 to
 open</em>list
 and
 note
 n
 as
 it
'
s
 parent</p>

<p>if
 m
 not
 in
 open<em>list
 and
 m
 not
 in
 closed</em>list:
open_list.add(m)</p>

<p>parents[m] = n</p>

<p>g[m] = g[n] +
 int
(weight)
#
 otherwise
,
 check
 if
 it
'
s
 quicker
 to
 first
 visit
 n
,
then
 m
64</p>

<p>#
 and
 if
 it
 is
,
 update
 parent
 data
 and
 g
 data
#
 and
 if
 the
 node
 was
 in
 the
 closed<em>list
,
 move
 it
 to
open</em>list
else
:
if
 g[m] &gt; g[n] + weight:
g[m] = g[n] + weight
parents[m] = n
if
 m
 in
 closed<em>list:
closed</em>list.remove(m)
open<em>list.add(m)
#
 remove
 n
 from
 the
 open</em>list
,
 and
 add
 it
 to
 closed_list</p>

<p>#
 because
 all
 of
 his
 neighbors
 were
 inspected</p>

<p>open_list.remove(n)</p>

<p>closed_list.add(n)
print
(
'
Path
 does
 not
 exist
!
'
)
return
 None
â€¹
In the
behaviourPath()
function, as in Dijkstra algorithm, vectors with source
no de and target no des that describ e the p ossible b ehaviour are s tore d. Being
aware of the map, it is p oss ible to know the feasible mano euvres from a sp ecic</p>

<p>no de. Taking into account the path, if the car is in the p osition of an "input"</p>

<p>no de, for example "RIGHT
INPUT" no de, and the destination no de after
the intersection is on the right and it is stored in "RIGHT
OUTPUT", the
b ehaviour of the car is to turn right and it is sent to
MovCarProcess
. The
for
statement is used to iterate over the path vector in order to understand which</p>

<p>b ehaviour the car must follow.
65</p>

<p>6 Parking
One of the most dicult tasks, in particular for amateur drivers, is parking (Fig.30).
Finding a parking sp ot, enough large for the drivers' car, is one of the most common</p>

<p>issues in cities or p opular places. Once a suitable parking sp ot is found, the second</p>

<p>problem is to start a mano euvre avoiding collisions and trying to b e fast in order</p>

<p>to not disturb the surrounding trac. Autonomous Parking system helps drivers to
shirk these problems and autonomous vehicle to fulll the mano euvre. Nowadays,
parking systems use cameras and sensors to analyze the surrounding environment</p>

<p>and to compute the mano euvre. Automatic parking has several di
erent control</p>

<p>approaches available [30]: some approaches use sp ecic servers that mark o ccupied</p>

<p>parking sp ot, others use came ra to analyze the sp ot and understand if there is a
vehicle inside, others use LiDar sensors or Ultrasonic sensors. Due to high customer
value, car companies are developing parking systems and functions related to parking</p>

<p>assistance [31].
Figure 30: Autonomous parking.
6.1 Parking at comp etition
Parking is an im p ortant task in autonomous vehicle and it is mandatory for the
comp etition. In the track is known that parking station is comp osed by two parking</p>

<p>sp ots. The car is provided by a camera, a p ointing LiDar and Ultras onic sensor and</p>

<p>the team exploits them to accomplish the parking mano euvre.
Team Politron develops Parking mano euvre (Fig.31) in
MovCarProcess
making
use of
SignDetectionProcess
.
SignDetectionProcess
exploits the camera to recognize
trac sign and se nds a 
ag that corresp onds to the detecte d sign. Parking mano eu-</p>

<p>vre starts when
SignDetectionProcess
detects the parking sign: when the parking
sign is detec ted, the HC-SR04 is used to evaluate if the rst parking sp ot is empty or
is busy. Next, it scans the right-hand side of the car and saves the obstacle p osition,
then it starts the mano euvre dep ending on it. At the same time, the car uses IMU</p>

<p>sensor to go straight. If the rst lot is empty, counters are used to mark the di
erent</p>

<p>steps of the parking mano euvre:
1.
 The car places its middle lateral axis p erp endicular to the last line delimiting
the parking lot.
66</p>

<p>2.
 The car steers the whee ls and starts the backward motion.
3.
 The car steers again the wheels to have them s traight and to complete the
parking mano euvre.
4.
 If the car is not to o near to anothe r car, it go es straight in order to reach the
center of the parking sp ot. If it is to o near, LiDar detects it and the c ar s tops .
5.
 The car starts the backward motion.</p>

<p>6.
 The car steers the whee ls and starts the forward motion.</p>

<p>7.
 The car steers again the wheels in the other side to go back into the lane.
Figure 31: Autonomous parking in the comp etition.
If the rst parking sp ot is busy, the car go es straight until HC-SR04 sensor
detects an empty parking sp ot and immediately after the se ven steps written ab ove
are executed.
#
 PARKING
 MANOUVRE
elif
 PARKING_MANOEUVRE == True:
cnt = cnt + 1</p>

<p>if
 cnt == 1:
print
(
"
Parking
 manoeuvre
 initiating
...
"
)</p>

<p>self.CarDetected = 2
actual<em>yaw = self.Yaw
if
 cnt &lt;= 10:
#
go
 straight
valore = self.</em>straight<em>correction(actual</em>yaw)
elif
 cnt &gt; 17
 and
 cnt &lt; 27:
#
 first
 slot
 is
 empty
,
 the
 car
 is
 in
 correspondence
 of
 the
 second
slot
,
 it
 stops
valore = 999
elif
 cnt &gt;= 27
 and
 cnt &lt; 60:
 #
diminuito
 di
 10
#
 turn
 the
 wheels
 and
 it
 starts
 backward
 motion</p>

<p>valore = 1000
elif
 cnt &gt;= 60
 and
 cnt &lt; 93:
 #
diminuito
 di
 20
#
 turn
 the
 wheel
 to
 the
 other
 side
 and
 it
 starts
 backward
 motion</p>

<p>valore = 1001
elif
 cnt &gt;= 93
 and
 cnt &lt; 130
 and
 not
 SECOND_PARKING:
#
if
 there
 is
 a
 car
 in
 front
 and
 the
 team
 car
 is
 near
 it
 must
 stops</p>

<p>if
 self.Lidar == 99:
valore = 999
else
:
67</p>

<p>#
otherwise
 go
 straight
 in
 order
 to
 reach
 the
 center
 of
 the
 slot
valore = self.<em>straight</em>correction(actual<em>yaw)
elif
 cnt &gt;= 93
 and
 cnt &lt; 130
 and
 SECOND</em>PARKING:
if
 cnt &lt; 100:
#
if
 the
 first
 slot
 in
 busy
,
 go
 straight
valore = self.<em>straight</em>correction(actual_yaw)
else
:
valore = 999
elif
 cnt &gt;= 130
 and
 cnt &lt; 155:
#
 Stop
 in
 the
 parking
 slot</p>

<p>valore = 999
elif
 cnt &gt;= 155
 and
 cnt &lt; 170:
#
 Start
 the
 backward
 motion</p>

<p>valore = 1000
elif
 cnt &gt;= 170
 and
 cnt &lt; 180:
#
 Turn
 wheel
 and
 start
 forward
 motion</p>

<p>valore = 2001
elif
 cnt &gt;= 180:
cnt = 0</p>

<p>#
end
 PARKING
 manoeuvre</p>

<p>PARKING_MANOEUVRE = False</p>

<p>NORMAL = True</p>

<p>print
(
"
Parking
 manoeuvre
 completed
"
)
else
:
pass
The parking algorithm used in the comp etition is not ge ne ral; mano euvres are set
for the parking sp ot of the comp etition track, the y are preset knowing the size of
the parking sp ot. Furthermore, parallel parking is the only one that the car is able
to fulll.
68</p>

<p>7 Conclusion and future development
This work has dealt with the participation to the Bosch Future Mobility challenge.
It is an inte rnational autonomous driving and connectivity c omp etition for bachelor</p>

<p>and master students created by Bos ch Romania. Nowadays, self driving is one of the</p>

<p>most dis cussed topics and it de velops faster and faster. Students join the challenge</p>

<p>to learn more ab out autonomous driving and to develop the concerning algorithms.
The author was memb er of team Politron and together with the other colleagues of
the team decided to fac e the challenge. They win the Bes t New participating Team</p>

<p>award.
This thesis dee p ens the deployment of a tra jectory planning and path planning
system for the autonomous 1:10 scaled ve hicle of the comp etition. To do that,
TF-Luna LiDar sensor and HC-SR04 Ultrasonic Se ns or Mo dule were used. In qual-
ications, the former was able to detect obstacle but it was p ositioned in the wrong</p>

<p>way, it was p ointing to the ground, so the car continuously stops b ecause it detects a</p>

<p>wall that did not exist; instead in nals, the car stops b ecause LiDar laser p oints on</p>

<p>the road sign p ost concluding the overtake mano euvre. HC -SR 04 Ultrasonic Sensor
Mo dule, b oth in qualications and in nals, had no problem to detect right-side
obstacle.
At comp etition, the team chose a predened path that the c ar must follow to
fulll all the tasks required. Tra jec tory planning algorithms were used in the com-</p>

<p>p etition and the car followed the chosen path for b oth the technical and the sp eed
race as the team exp ec te d. Firstly, algorithms saves the graph that corresp onds to
the comp etition track and connecting to the GPS, they were able to understand</p>

<p>the p osition of the car on the track. Next, the b ehaviour of the car is se t: if the</p>

<p>no des are in the intersection, the car must accomplish the b ehavior selected for that</p>

<p>certain no de in order to follow the chosen path, otherwise the car must follow the
lane.
Path planning algorithms nds the shortest path using the data from the saved
graph of the comp etition track. Dijkstra algorithm pro cesses the shortest path from
source to adjacent no des, the latter is called intermediate no de. Next, it rep eats the</p>

<p>action b etween intermediate no de to its adjacent no des. A* algorithm nds a path</p>

<p>that avoids collisions resp ecting car limits and constraints when the car structure and
the p osition of obstacles are known. It uses a function that indic ate s the directions
to follow and it nishes when the end p oint is reached. According to the function, if</p>

<p>a step is reasonable the algorithm p erforms it. Heuristic metho ds are the base of the</p>

<p>A* algorithm that nds the b est p oss ible solution. Both Dijkstra and A* algorithm</p>

<p>save the path and connecting to the GPS, the car knows its p osition on the track
and it is able to fulll the briefest path.
All things considered, tra jectory planning algorithms have demonstrated go o d
p erformance during the comp etition; in spite of this, further improvements can b e
made.
Tra jectory planning algorithms used in the comp etition and the path planning
subsequently implemented can b e used in the challenge thanks to the server connec-</p>

<p>tion give n by Bos ch Romania however, they must b e adapted to a real GPS to b e</p>

<p>applicable in re al world with hardware upgrade. Both tra jectory and path planning</p>

<p>are based on graphs comp osed by no des fundamental to nd the path. Mano euvres</p>

<p>in the intersection are set for the dimension of the track, a very go o d implementation
is an algorithm that provides a correct b ehaviour for di
erent kind of intersections.
69</p>

<p>The brain of the c ar is on the Raspb erry Pi, a single b oard computer whose
p erformance is not optimal for the required tasks. Moreover, the car is provided
by a xed PiC amera, a HC-SR04 Ultrasonic Sensor Mo dule and a TF-Luna LiDar</p>

<p>Mo dule. To improve the pro ject, a go o d idea is to implement a s ervomotor to</p>

<p>move the camera and have a b etter vis ual of the environment and to use a b etter</p>

<p>p erforming came ra in order to increas e accuracy in lane, trac sign, trac light
and ob ject detection. Another HC-SR04 Ultrasonic Sensor Mo dule put in front of
the camera can improve the functionality of the p ointing LiDar b ec aus e it has a</p>

<p>bigger radius to sense if an obstacle is approaching, like a p edestrian that wants to</p>

<p>cross the road. Instead of TF-Luna LiDar Mo dule, it can b e useful to employ a 360
Â°
LiDar that can improve in a particular way parking mano euvre.
70</p>

<p>References
[1]
 \Bosch invented for life," Do cumentation. [Online]. Available: https:
//b oschfuturemobility.com/do cume ntation- main/
[2]
 N. Deo and M. M. Trivedi, \Multi-mo dal tra jectory prediction of surround-
ing vehicles with maneuver based lstms," in
2018 IEEE Intel ligent Vehicles
Symposium (IV)
, 2018, pp. 1179{1184.
[3]
 S. Ab dallaoui, E.-H. Aglzim, A. Chaib e t, and A. Kribeche, \Thorough
review analysis of safe control of autonomous vehicles: Path planning and
navigation techniques,"
Energies
, vol. 15, no. 4, 2022. [Online]. Available:
https://www.mdpi.com/1996- 1073/15/4/1358
[4]
 J. M. Scanlon, K. D. Kusano, T. Danie l, C. Alderson, A. Ogle,
and T. Victor, \Waymo simulated driving b ehavior in reconstructed</p>

<p>fatal crashes within an autonomous vehicle op erating domain." [Online].</p>

<p>Available: https ://s torage.go ogleapis.com/waymo- uploads/les/do cuments/</p>

<p>Waymo- Simulated- Driving- Behavior- in- Rec ons truc te d- Collisions.p df
[5]
 B. Gringer, \History of the autonomous car." [Online]. Available: https:
//www.titlemax.com/resources/history- of- the- autonomous- car/
[6]
 J. Fayyad, M. A. Jaradat, D. Gruyer, and H. Na jjaran, \Deep
Learning Sensor Fusion for Autonomous Vehicle Pe rc eption and Lo calization:</p>

<p>A Review,"
Sensors 20
, 2020, no. 15: 4220. [Online]. Available :
https://doi.org/10.3390/s20154220
[7]
 S. Kuutti, R. Bowden, Y. Jin, P. Barb er, and S. Fallah, \A Survey of
Deep Learning Applications to Autonomous Vehicle Control,"
IEEE Trans-
actions on Intel ligent Transportation Systems
, vol. 22, 07 January 2020, doi:
10.1109/TITS.2019.2962338.
[8]
 J. M. Anderson, N. Kalra, K. D. Stanley, P. Sorensen, C. Samaras, and O. A.
Oluwatola, \Autonomous vehicle technology,"
RAND Corporation
, isbn: 978-
0-8330-8398-2.
[9]
 H. A. Ignatious, Hesham-El-Sayed, and M. Khan, \An overview of
sensors in Autonomous Vehicles,"
Elsevier B.V
, 2021. [Online]. Available:
www.sciencedirect.com
[10]
 Vargas, Jorge, S. Alswe iss, O. Toker, R. Razdan, , and J. Santos,
\An Overview of Autonomous Vehicles Sensors and Their Vulnerability to</p>

<p>Weather Conditions,"
Sensors 21
, 2021, no. 16: 5397. [Online]. Available:
https://doi.org/10.3390/s21165397
[11]
 U. Wandinger, \Intro duction to Lidar. In: Weitkamp, C. (eds) Lidar. Springer
Series in Optical Sciences,"
Springer, New York, NY
, vol. 102, 2005, isbn: 978-
0-387-40075-4. [Online]. Available: https ://doi.org/10.1007/0- 387- 25101- 4
1
[12]
 J. Liu, Q. Sun, Z. Fan, and Y. Jia, \Tof lidar development in autonomous
vehicle," in
2018 IEEE 3rd Optoelectronics Global Conference (OGC)
, 2018,
pp. 185{190, doi: 10.1109/OGC.2018.8529992.
71</p>

<p>[13]
 B. S. Lim, S. L. Keoh, and V. L. L. Thing, \Autonomous vehicle ultrasonic
sensor vulnerability and impact assessment," in
2018 IEEE 4th World Fo-
rum on Internet of Things (WF-IoT)
, 2018, pp. 231{236, doi: 10.1109/WF-
IoT.2018.8355132.
[14]
 \Complete Guide for Ultrasonic Sensor HC-SR04 with Ar-
duino." [Online]. Available: https ://randomnerdtutorials.com/
complete- guide- for- ultrasonic- sensor- hc- sr04/
[15]
 N. Ahmad, R. A. R. Ghazilla, , and N. M. Khairi, \Reviews on Various Ine rtial
Measurement Unit (IMU) Sensor Applications,"
International Journal of Signal
Processing Systems
, vol. 1, no. 2, 2013.
[16]
 K. Jo, J. Kim, D. Kim, C. Jang, and M. Sunwo o, \Development of autonomous
car|part i: Distributed system architecture and development pro cess,"
IEEE
Transactions on Industrial Electronics
, vol. 61, no. 12, pp. 7131{7140, 2014,
doi: 10.1109/TIE.2014.2321342.
[17]
 R. C. Shit and S. Sharma, \Lo calization for autonomous vehicle: Analysis of
imp ortance of iot network lo calization for autonomous vehicle applications," in</p>

<p>2018 International Conference on Applied Electromagnetics, Signal Processing</p>

<p>and Communication (AESPC)
, vol. 1, 2018, pp. 1{6.
[18]
 Fujii, Sae, A. Fujita, Umedu, Takaaki, Kaneda, Shigeru, Yamaguchi, Hirozumi,
Higashino, Teruo, Takai, and Mineo, \Co op erative vehicle p ositioning via v2v</p>

<p>communications and onb oard sensors," in
2011 IEEE Vehicular Technology
Conference (VTC Fal l)
, 2011, pp. 1{5.
[19]
 \NetworkX, Network Analysis in Python." [Online]. Available: https:
//networkx.org
[20]
 \Bosch future mobility challenge," GITHUB, Copyright (c ) 2019, Bosch
Engineering Center Cluj and BFMC organizers. All rights reserved. [Online].</p>

<p>Available: https://github.com/ECC- BFMC
[21]
 B. Siciliano, L. Sciavicco, L. Villani, and G. Oriolo,
Robotics, Model ling, Plan-
ning and control
. Springer-Verlag London Limited, 2009, isbn: 978-1-84628-
641-4, doi: 10.1007/978-1-84628-642-1.
[22]
 A. Trotta, \Tra jectory planner for autonomous vehicle," 2016, master Degree's
Thesis at Politecnico di Torino.
[23]
 A. E. Mahdawy and A. E. Mougy, \Path planning for autonomous vehicles
with dynamic lane mapping and obstacle avoidance." in
In Proceedings of the
13th International Conference on Agent s and Articial In tel ligence (ICAART
2021)
, vol. 1, 2021, pp. 431{438, isbn: 978-989-758-484-8.
[24]
 P. Mishrs, B. S. Sujith, and K. Mall, \A nove l path planning algorithm for
autonomous rob ot navigation," in
2010 International Conference on Computer
Applications and Industrial Electronics
, 2010, pp. 180{185.
[25]
 P. Ren, S. Chen, and H. Fu, \Intelligent path planning and obstacle avoid-
ance algorithms for autonomous vehicles bas ed on enhanced rrt algorithm," in
2021 6th International Conference on Communicat ion and Electron ics Systems
(ICCES)
, 2021, pp. 1868{1871.
72</p>

<p>[26]
 P. Bautista-Camino, A. I. Barranco-Gutierrez, I. Cervantes, M. Ro drguez-
Licea, J. Prado-Olivare z, and F. J. Perez-Pinal, \Lo cal path planning
for autonomous vehicles based on the natural b ehavior of the biological</p>

<p>action p erception motion."
MDPI, Wiseman Yair and Antonio Cano-
Ortega
, 27 February 2022, energies 2022, 15,1769. [Online]. Available:
https://doi.org/10.3390/en15051769
[27]
 Qing, Guo, Zheng, Zhang, Yue, and Xu, \Path-planning of automated guided
vehicle based on improved dijkstra algorithm," in
2017 29th Chinese Con-
trol And Decision Conference (CCDC)
, 2017, pp. 7138{7143, doi: 10.1109/C-
CDC.2017.7978471.
[28]
 W. Yijing, L. Zhengxuan, Z. Zhiqiang, and L. Zheng, \Lo cal path planning of
autonomous vehicles based on a algorithm with equal-step sampling," in
2018
37th Chinese Cont rol Conference (CCC)
, 2018, pp. 7828{7833.
[29]
 \A* search algorithm,"
Â©
2013-2022 Stack Abuse. [Online]. Available: https:
//stackabuse.com/courses/graphs- in- python- theory- and- implementation/</p>

<p>lessons/a- star- s earch- algorithm/
[30]
 D. Pe re z-Morales, S. Domnguez-Quijada, O. Kermorgant, and P. Martinet.,
\8th workshop on planning, p erception and navigation for intelligent vehicles</p>

<p>at ieee int. conf. on intelligent transp ortation systems," in
Autonomous parking
using a sensor based approach.
, Nov 2016, Rio de Janeiro, Brazil.
[31]
 P.omb erg, \Device-less remote parking based on ultrasonic sensor detec-
tion," in
Virtual Leash
, 2019.
73</p>

<p>Ringraziamenti
Vorrei ringraziare la mia famiglia p er avermi sempre sos tenuto, p er essermi stata
vicina nei momenti di c ris i e p er avermi dato la p ossibilita di crescere face ndomi
intraprendere il mio p ercorso universitario dall'altra parte dell'Italia. Tra loro,
vorrei citare Ginevra che nell'ultimo anno, nonche il suo primo anno di vita, e
riuscita a p ortare dentro di me una felicita immensa.
Inoltre vorre i ringraziare tutti i ragazzi del Team Politron e il professore Stefano
Malan p er l'esp erienza che mi hanno p ermes so di vivere. La BFMC 2022 rimarra
sempre un ricordo b ellissimo. Vorrei anche ringraziare il Politecnico di Torino e
tutte le p ersone che ci lavorano p er avermi fatto m aturare e comunque p er essere
stati parte di un b el p o' di anni della mia vita.
Inne vorrei ringraziare, Daniel p er avermi fatto rialzare tutte le volte che s ono
caduta, p er essermi stato vicino, p er avermi sopp ortato e p er avermi cucinato ogni
volta che non l'avrei fatto. Le Trigone che nonostante la distanza c i sono sempre, e
dico sempre, state. Le mie amiche e i miei amici "la famiglia che ti s cegli" che
hanno creduto in me tra cui Pipino p er essere stato il miglior Cotino di sempre.
Rinomino Daniel e Cuchina p er ringraziarli partic olarme nte p er la pazienza che
hanno avuto durante la scrittura di questa tesi, p er avermi aiutata e sopp ortata.
E come non p otrei ringraziare Hardin, il nostro piccolo e p e loso coniglietto che si e
subito le spiegazioni della parte teorica di molte materie.
74</p>
